===== SCRIPT SNAPSHOT BEGIN =====
#!/usr/bin/env python
# -*- coding: utf-8 -*-
from __future__ import annotations

import atexit
import json
import math
import random
import sys
import time
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Dict, Iterator, List, Optional, Tuple, cast

import numpy as np
import torch
import torchaudio
from torch import nn
from torch.nn import functional as F
from torch.nn.attention import SDPBackend, sdpa_kernel
from torch.nn.utils.rnn import pad_sequence
from torch.utils.data import DataLoader, Dataset, Sampler
from torchaudio.compliance import kaldi
from tqdm import tqdm

from utils.specAug import SpecAugment


@dataclass
class ASRConfig:
    train_manifest: str
    dev_manifest: str
    cmvn_path: str = "dataset/cmvn.npy"
    sample_rate: int = 16000
    num_mel_bins: int = 80
    frame_length_ms: float = 25.0
    frame_shift_ms: float = 10.0
    dither: float = 0.0

    # ===== [MOD] 贴近 WeNet/常见 Conformer Base 形状 =====
    encoder_dim: int = 256          # d_model
    ffn_dim: int = 2048             # FFN hidden dim（你之前误当 hidden_size 用了）
    num_heads: int = 4              # 256/4=64
    layers: int = 12                # 2 层太薄了，先上 12（显存紧张再降到 6）

    dropout: float = 0.1

    batch_size: Optional[int] = None
    max_frames_per_batch: Optional[int] = 8192
    num_epochs: int = 100
    learning_rate: float = 5e-4     # WeNet 常见量级
    weight_decay: float = 1e-5
    max_grad_norm: float = 5.0

    num_workers: int = 4
    prefetch_factor: int = 2
    bucket_size: int = 100
    pin_memory: bool = True
    shuffle: bool = True
    persistent_workers: bool = True

    # ===== [MOD] dev 不要 CPU 重计算：抽样算 CER =====
    eval_max_batches: Optional[int] = None       # 只想更快就设 200/300
    eval_cer_max_batches: int = 50               # 只用前 50 个 batch 算 CER（够看趋势）
    train_cer_log_interval: int = 200            # 训练中每 N step 抽样打印 CER

    # ===== [MOD] SpecAug 放到 main 里实例化，避免全局变量 =====
    use_specaug_prob: float = 0.1

    # ===== [MOD] WarmupLR（WeNet 常配 warmuplr / warmup_steps 例如 12000）=====
    warmup_steps: int = 12000


# ======================
# 动态 batch 采样器
# ======================
class LengthBucketBatchSampler(Sampler[List[int]]):
    def __init__(
        self,
        lengths: List[int],
        batch_size: Optional[int] = None,
        max_frames_per_batch: Optional[int] = None,
        shuffle: bool = True,
        bucket_size: int = 100,
    ):
        super().__init__(None)
        assert (batch_size is None) ^ (max_frames_per_batch is None), \
            "Exactly one of batch_size or max_frames_per_batch should be provided."
        self.lengths = lengths
        self.batch_size = batch_size
        self.max_frames_per_batch = max_frames_per_batch
        self.shuffle = shuffle
        self.bucket_size = bucket_size
        self._indices = list(range(len(lengths)))
        self.g = torch.Generator().manual_seed(torch.seed())

    def __iter__(self) -> Iterator[List[int]]:
        if self.shuffle:
            indices = [self._indices[i] for i in torch.randperm(len(self._indices), generator=self.g).tolist()]
        else:
            indices = self._indices

        buckets = [indices[i:i + self.bucket_size] for i in range(0, len(indices), self.bucket_size)]
        for b in buckets:
            b.sort(key=lambda i: self.lengths[i])

        flat = [i for b in buckets for i in b]

        if self.batch_size is not None:
            for i in range(0, len(flat), self.batch_size):
                yield flat[i:i + self.batch_size]
        else:
            current: List[int] = []
            cur_frames = 0
            max_frames = cast(int, self.max_frames_per_batch)
            for i in flat:
                length = int(self.lengths[i])
                if len(current) > 0 and (cur_frames + length) > max_frames:
                    yield current
                    current, cur_frames = [], 0
                current.append(i)
                cur_frames += length
            if len(current) > 0:
                yield current

    def __len__(self) -> int:
        if self.batch_size is not None:
            return math.ceil(len(self.lengths) / self.batch_size)
        avg = (sum(self.lengths) / max(1, len(self.lengths)))
        return max(1, int(sum(self.lengths) / max(cast(int, self.max_frames_per_batch), cast(int, avg))))


# ======================
# 字级 tokenizer（0 留给 CTC blank）
# ======================
class CharTokenizer:
    def __init__(self, char2id: Dict[str, int]) -> None:
        self.blank_id: int = 0
        self.char2id: Dict[str, int] = char2id
        self.id2char: List[str] = ["<blank>"] + [ch for ch, _ in sorted(char2id.items(), key=lambda x: x[1])]

    @classmethod
    def build_from_jsonl(cls, manifest_path: str) -> "CharTokenizer":
        chars: set[str] = set()
        path = Path(manifest_path)
        with path.open("r", encoding="utf-8") as f:
            for line in f:
                if not line.strip():
                    continue
                item = json.loads(line)
                txt = item.get("txt") or item.get("text")
                if txt is None:
                    raise ValueError("每行 json 需要包含 'txt' 或 'text' 字段")
                for ch in txt:
                    if ch.isspace():
                        continue
                    chars.add(ch)
        char2id: Dict[str, int] = {}
        next_id = 1
        for ch in sorted(chars):
            char2id[ch] = next_id
            next_id += 1
        return cls(char2id)

    @property
    def vocab_size(self) -> int:
        return 1 + len(self.char2id)

    def encode(self, text: str) -> List[int]:
        ids: List[int] = []
        for ch in text:
            if ch.isspace():
                continue
            idx = self.char2id.get(ch)
            if idx is None:
                continue
            ids.append(idx)
        return ids

    def decode_ids(self, ids: List[int]) -> str:
        chars: List[str] = []
        for idx in ids:
            if idx == self.blank_id:
                continue
            if 0 <= idx < len(self.id2char):
                ch = self.id2char[idx]
                if ch not in ("<blank>", "<unk>"):
                    chars.append(ch)
        return "".join(chars)


class JsonlASRDataset(Dataset):
    def __init__(
        self,
        manifest_path: str,
        tokenizer: CharTokenizer,
        config: ASRConfig,
        compute_lengths: bool = False,
        use_precomputed_fbank: bool = False
    ) -> None:
        super().__init__()
        self.manifest_path = Path(manifest_path)
        self.tokenizer = tokenizer
        self.sample_rate = config.sample_rate
        self.num_mel_bins = config.num_mel_bins
        self.frame_length_ms = config.frame_length_ms
        self.frame_shift_ms = config.frame_shift_ms
        self.dither = config.dither
        self.compute_lengths = compute_lengths
        self.use_precomputed_fbank = use_precomputed_fbank

        self.entries: List[Dict[str, Any]] = []
        self.lengths: List[int] = []

        with self.manifest_path.open("r", encoding="utf-8") as f:
            for line in f:
                if not line.strip():
                    continue
                item = json.loads(line)
                txt = item.get("txt") or item.get("text")
                if txt is None:
                    raise ValueError("清单每行必须包含 'txt'/'text'")

                if use_precomputed_fbank:
                    feat = item.get("feat")
                    if feat is None:
                        raise ValueError("使用预处理 fbank 时，清单每行必须包含 'feat'")
                    self.entries.append({"feat": feat, "txt": txt})

                    if compute_lengths:
                        feat_frames = int(item.get("feat_frames", 0) or 0)
                        self.lengths.append(feat_frames if feat_frames > 0 else 100)
                else:
                    wav = item.get("wav")
                    if wav is None:
                        raise ValueError("清单每行必须包含 'wav'")
                    self.entries.append({"wav": wav, "txt": txt})

                    if compute_lengths:
                        duration_sec = float(item.get("length", 0.0) or 0.0)
                        if duration_sec > 0:
                            duration_ms = duration_sec * 1000.0
                            feat_frames = int((duration_ms - self.frame_length_ms) / self.frame_shift_ms) + 1
                            self.lengths.append(max(1, feat_frames))
                        else:
                            self.lengths.append(100)

        if not self.entries:
            raise RuntimeError(f"{self.manifest_path} 为空")

    def __len__(self) -> int:
        return len(self.entries)

    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor, str]:
        item = self.entries[idx]
        text: str = item["txt"]

        if self.use_precomputed_fbank:
            feat_path = item["feat"]
            if feat_path.endswith(".npy"):
                feat = torch.from_numpy(np.load(feat_path))
            else:
                feat = torch.load(feat_path, weights_only=False)
            if feat.dtype != torch.float32:
                feat = feat.float()
        else:
            wav_path = item["wav"]
            waveform, sr = torchaudio.load(wav_path)
            if waveform.size(0) > 1:
                waveform = waveform.mean(dim=0, keepdim=True)
            if sr != self.sample_rate:
                resampler = torchaudio.transforms.Resample(orig_freq=sr, new_freq=self.sample_rate)
                waveform = resampler(waveform)
                sr = self.sample_rate
            waveform = waveform * (1 << 15)
            feat = kaldi.fbank(
                waveform,
                num_mel_bins=self.num_mel_bins,
                frame_length=self.frame_length_ms,
                frame_shift=self.frame_shift_ms,
                dither=self.dither,
                energy_floor=0.0,
                sample_frequency=float(sr),
            )
        target_ids = torch.tensor(self.tokenizer.encode(text), dtype=torch.long)
        return feat, target_ids, text


def asr_collate_fn(batch: List[Tuple[torch.Tensor, torch.Tensor, str]]) -> Dict[str, Any]:
    # ===== [MOD] 过滤空 target（CTC 会炸）=====
    filtered: List[Tuple[torch.Tensor, torch.Tensor, str]] = []
    for feat, tgt, txt in batch:
        if tgt.numel() == 0:
            continue
        filtered.append((feat, tgt, txt))
    if not filtered:
        # 返回一个占位（上层会跳过）
        return {"_empty": True}

    feats, targets, texts = zip(*filtered)
    feat_lengths = torch.tensor([f.size(0) for f in feats], dtype=torch.long)
    padded_feats = pad_sequence(feats, batch_first=True)  # (B, T, D)

    target_lengths = torch.tensor([t.size(0) for t in targets], dtype=torch.long)
    concatenated_targets = torch.cat(targets, dim=0)

    return {
        "feats": padded_feats,
        "feat_lengths": feat_lengths,
        "max_feat_lengths": int(feat_lengths.max().item()),
        "targets": concatenated_targets,
        "target_lengths": target_lengths,
        "texts": list(texts),
    }


# ===== [MOD] WeNet 同款 Conv2dSubsampling4（时长 /4），并正确更新长度 =====
# 参考：WeNet 的 Conv2dSubsampling4 实现与 mask 切片方式 :contentReference[oaicite:6]{index=6}
class Conv2dSubsampling4(nn.Module):
    def __init__(self, idim: int, odim: int):
        super().__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(1, odim, kernel_size=3, stride=2),
            nn.ReLU(),
            nn.Conv2d(odim, odim, kernel_size=3, stride=2),
            nn.ReLU(),
        )
        # freq 维度从 idim -> (((idim-1)//2 -1)//2)（WeNet 的写法）
        out_freq = (((idim - 1) // 2 - 1) // 2)
        self.out = nn.Linear(odim * out_freq, odim)

    @staticmethod
    def _out_len(lens: torch.Tensor) -> torch.Tensor:
        # conv2d valid padding: out = floor((L-3)/2 + 1) = floor((L-1)/2)
        l1 = torch.div(lens - 1, 2, rounding_mode="floor")
        l2 = torch.div(l1 - 1, 2, rounding_mode="floor")
        return torch.clamp(l2, min=0)

    def forward(self, x: torch.Tensor, x_lens: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        # x: (B, T, F)
        x = x.unsqueeze(1)  # (B, 1, T, F)
        x = self.conv(x)    # (B, C, T', F')
        b, c, t, f = x.size()
        x = x.transpose(1, 2).contiguous().view(b, t, c * f)  # (B, T', C*F')
        x = self.out(x)  # (B, T', C)
        out_lens = self._out_len(x_lens)
        return x, out_lens


class CTCConformer(nn.Module):
    def __init__(
        self,
        input_dim: int,
        vocab_size: int,
        encoder_dim: int,
        ffn_dim: int,
        num_layers: int,
        num_heads: int,
        dropout: float,
    ) -> None:
        super().__init__()
        # ===== [MOD] WeNet 风格：Conv2dSubsampling4 把 80 -> encoder_dim，并把 T / 4 =====
        self.subsampling = Conv2dSubsampling4(idim=input_dim, odim=encoder_dim)

        # ===== [MOD] 正确使用 torchaudio Conformer：input_dim=d_model，ffn_dim=FFN hidden =====
        self.conformer = torchaudio.models.Conformer(
            input_dim=encoder_dim,
            num_layers=num_layers,
            ffn_dim=ffn_dim,
            dropout=dropout,
            depthwise_conv_kernel_size=15,
            num_heads=num_heads,
        )
        self.ctc_linear = nn.Linear(encoder_dim, vocab_size)

    def forward(self, feats: torch.Tensor, feat_lengths: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        x, out_lens = self.subsampling(feats, feat_lengths)     # (B, T', C)
        x, out_lens2 = self.conformer(x, out_lens)              # (B, T', C)
        logits = self.ctc_linear(x)                             # (B, T', V)
        return logits, out_lens2


def ctc_greedy_decode_ids(
    logit_batch: torch.Tensor,
    lengths: torch.Tensor,
    blank_id: int,
) -> List[List[int]]:
    # 只用于“抽样算 CER”，不要拿它当训练瓶颈
    with torch.no_grad():
        pred = torch.argmax(F.log_softmax(logit_batch, dim=-1), dim=-1)  # (B, T)
    results: List[List[int]] = []
    bsz = pred.size(0)
    for b in range(bsz):
        T = int(lengths[b].item())
        seq = pred[b, :T].tolist()
        collapsed: List[int] = []
        prev = blank_id
        for idx in seq:
            if idx != blank_id and idx != prev:
                collapsed.append(idx)
            prev = idx
        results.append(collapsed)
    return results


def char_edit_distance(ref: str, hyp: str) -> int:
    # 简单 DP：只在抽样 batch 上跑，避免 dev 变“CPU 大赛”
    ref_chars = list(ref)
    hyp_chars = list(hyp)
    n = len(ref_chars)
    m = len(hyp_chars)
    if n == 0:
        return m
    if m == 0:
        return n
    dp = [[0] * (m + 1) for _ in range(n + 1)]
    for i in range(n + 1):
        dp[i][0] = i
    for j in range(m + 1):
        dp[0][j] = j
    for i in range(1, n + 1):
        ri = ref_chars[i - 1]
        for j in range(1, m + 1):
            cost = 0 if ri == hyp_chars[j - 1] else 1
            dp[i][j] = min(
                dp[i - 1][j] + 1,
                dp[i][j - 1] + 1,
                dp[i - 1][j - 1] + cost,
            )
    return dp[n][m]


@torch.no_grad()
def evaluate_ctc(
    model: nn.Module,
    dataloader: DataLoader,
    tokenizer: CharTokenizer,
    device: torch.device,
    ctc_loss_fn: nn.CTCLoss,
    cmvn: Optional[torch.Tensor],
    max_batches: Optional[int],
    cer_max_batches: int,
) -> Tuple[float, float, Dict[str, float]]:
    model.eval()
    total_loss = 0.0
    total_frames = 0

    # CER 只对前 cer_max_batches 计算
    total_edit = 0
    total_chars = 0

    blank_cnt = 0
    frame_cnt = 0
    hyp_len_sum = 0
    hyp_cnt = 0

    invalid_cnt = 0
    seen_cnt = 0

    for bi, batch in enumerate(tqdm(dataloader, ncols=100, desc="dev", leave=False)):
        if batch.get("_empty", False):
            continue
        if max_batches is not None and bi >= max_batches:
            break

        feats = batch["feats"].to(device, non_blocking=True)
        feat_lengths = batch["feat_lengths"].to(device, non_blocking=True)
        targets = batch["targets"].to(device, non_blocking=True)
        target_lengths = batch["target_lengths"].to(device, non_blocking=True)
        texts = batch["texts"]

        if cmvn is not None:
            feats = (feats - cmvn[0]) / cmvn[1]

        logits, out_lens = model(feats, feat_lengths)  # (B, T', V), (B,)
        log_probs = F.log_softmax(logits.float(), dim=-1).transpose(0, 1)  # (T', B, V)

        # invalid 样本统计：T' < U 时 CTCLoss 可能 inf，zero_infinity 会吞掉梯度 :contentReference[oaicite:7]{index=7}
        invalid = (out_lens < target_lengths).sum().item()
        invalid_cnt += int(invalid)
        seen_cnt += int(out_lens.numel())

        loss = ctc_loss_fn(log_probs, targets, out_lens, target_lengths)
        # ===== [MOD] reduction='sum'，再除以 batch（贴 WeNet CTC 写法）:contentReference[oaicite:8]{index=8} =====
        loss = loss / max(1, feats.size(0))

        batch_frames = int(out_lens.sum().item())
        total_loss += float(loss.item()) * batch_frames
        total_frames += batch_frames

        # debug：blank 比例 & 平均 hyp 长度（用 greedy 的 argmax 统计）
        pred = torch.argmax(logits, dim=-1)  # (B, T')
        blank_cnt += int((pred == tokenizer.blank_id).sum().item())
        frame_cnt += int(pred.numel())

        if bi < cer_max_batches:
            pred_id_seqs = ctc_greedy_decode_ids(logits, out_lens, tokenizer.blank_id)
            for ref_text, pred_ids in zip(texts, pred_id_seqs):
                hyp_text = tokenizer.decode_ids(pred_ids)
                ref = "".join(ch for ch in ref_text if not ch.isspace())
                total_edit += char_edit_distance(ref, hyp_text)
                total_chars += len(ref)
                hyp_len_sum += len(hyp_text)
                hyp_cnt += 1

    cer = (total_edit / total_chars) if total_chars > 0 else 0.0
    loss_per_frame = (total_loss / total_frames) if total_frames > 0 else 0.0
    debug = {
        "blank_frac": (blank_cnt / frame_cnt) if frame_cnt > 0 else 0.0,
        "avg_hyp_len": (hyp_len_sum / hyp_cnt) if hyp_cnt > 0 else 0.0,
        "invalid_ratio": (invalid_cnt / seen_cnt) if seen_cnt > 0 else 0.0,
    }
    return cer, loss_per_frame, debug


# ===== [MOD] WeNet/Transformer 常用 WarmupLR（step 级调度，别用 Plateau 看心情）=====
class WarmupLR:
    def __init__(self, optimizer: torch.optim.Optimizer, warmup_steps: int):
        self.optimizer = optimizer
        self.warmup_steps = max(1, int(warmup_steps))
        self.step_num = 0
        self.base_lrs = [group["lr"] for group in optimizer.param_groups]

    def step(self) -> float:
        self.step_num += 1
        s = float(self.step_num)
        w = float(self.warmup_steps)
        scale = (w ** 0.5) * min(s ** -0.5, s * (w ** -1.5))
        for lr0, group in zip(self.base_lrs, self.optimizer.param_groups):
            group["lr"] = lr0 * scale
        return self.optimizer.param_groups[0]["lr"]


def train_one_epoch(
    model: nn.Module,
    dataloader: DataLoader,
    optimizer: torch.optim.Optimizer,
    lr_sched: WarmupLR,
    ctc_loss_fn: nn.CTCLoss,
    device: torch.device,
    max_grad_norm: float,
    tokenizer: CharTokenizer,
    cmvn: Optional[torch.Tensor],
    specaug: Optional[SpecAugment],
    specaug_prob: float,
    cer_log_interval: int,
) -> float:
    model.train()
    total_loss = 0.0
    total_frames = 0

    total_edit = 0
    total_chars = 0

    invalid_cnt = 0
    seen_cnt = 0

    for step, batch in enumerate(tqdm(dataloader, ncols=100, desc="train", leave=False)):
        if batch.get("_empty", False):
            continue
        feats = batch["feats"].to(device, non_blocking=True)
        feat_lengths = batch["feat_lengths"].to(device, non_blocking=True)
        targets = batch["targets"].to(device, non_blocking=True)
        target_lengths = batch["target_lengths"].to(device, non_blocking=True)

        if cmvn is not None:
            feats = (feats - cmvn[0]) / cmvn[1]
        if specaug is not None and random.random() < specaug_prob:
            feats = specaug(feats)

        optimizer.zero_grad(set_to_none=True)

        with torch.autocast(device_type="cuda", dtype=torch.bfloat16):
            logits, out_lens = model(feats, feat_lengths)  # (B, T', V), (B,)
            log_probs = F.log_softmax(logits.float(), dim=-1).transpose(0, 1)

        invalid = (out_lens < target_lengths).sum().item()
        invalid_cnt += int(invalid)
        seen_cnt += int(out_lens.numel())

        loss = ctc_loss_fn(log_probs, targets, out_lens, target_lengths)
        loss = loss / max(1, feats.size(0))  # [MOD] 同 evaluate

        loss.backward()
        if max_grad_norm > 0.0:
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)
        optimizer.step()
        lr_sched.step()

        batch_frames = int(out_lens.sum().item())
        total_loss += float(loss.item()) * batch_frames
        total_frames += batch_frames

        # ===== [MOD] 训练 CER 只抽样算，别每步 CPU DP =====
        if cer_log_interval > 0 and (step % cer_log_interval == 0):
            with torch.no_grad():
                pred_id_seqs = ctc_greedy_decode_ids(logits, out_lens, tokenizer.blank_id)
                # 这里只用当前 batch 的 targets 做个粗估
                offset = 0
                for b, pred_ids in enumerate(pred_id_seqs):
                    cur_len = int(target_lengths[b].item())
                    tgt_ids = targets[offset:offset + cur_len].tolist()
                    offset += cur_len
                    ref_text = tokenizer.decode_ids(tgt_ids)
                    hyp_text = tokenizer.decode_ids(pred_ids)
                    total_edit += char_edit_distance(ref_text, hyp_text)
                    total_chars += len(ref_text)

    train_loss = (total_loss / total_frames) if total_frames > 0 else 0.0
    train_cer = (total_edit / total_chars) if total_chars > 0 else 0.0
    inv_ratio = (invalid_cnt / seen_cnt) if seen_cnt > 0 else 0.0
    print(f"train_loss: {train_loss:.3f} train_cer(sampled): {train_cer:.3f} invalid_ratio: {inv_ratio:.2%}")
    return train_loss


def dynamic_pre_compile(
        train_loader: DataLoader,
        device: torch.device,
        model: nn.Module,
        max_T: int,
        min_T: int,  # [MOD] 新增：最小动态长度
) -> nn.Module:
    warmup_batch = next(iter(train_loader))
    if warmup_batch.get("_empty", False):
        return model

    warmup_feats = warmup_batch["feats"].to(device)  # (B, T, D)
    warmup_feat_lengths = warmup_batch["feat_lengths"].to(device)

    # [MOD] Conv2dSubsampling4(k=3,s=2) 在很短的 T 上会触发 shape guard；
    #      你之前 min=1 让编译器要求 [1,max_T] 全区间都合法，于是炸。
    #      把 min 收紧到 >=11（或更大）即可。
    safe_min_T = int(max(11, min_T))
    safe_max_T = int(max(safe_min_T, max_T))

    torch._dynamo.mark_dynamic(warmup_feats, 1, min=safe_min_T, max=safe_max_T)

    # [MOD] 编译失败就直接回退到 eager，不要卡死训练
    try:
        model = torch.compile(model)
        with torch.no_grad():
            _ = model(warmup_feats, warmup_feat_lengths)
        print(f"[compile] ok: dynamic T in [{safe_min_T}, {safe_max_T}]")
        return model
    except torch.fx.experimental.symbolic_shapes.ConstraintViolationError as e:
        print(f"[compile] ConstraintViolationError, fallback to eager. err={e}")
        return model


def main() -> None:
    run_dir = Path("logs") / time.strftime("%Y%m%d-%H%M%S")
    run_dir.mkdir(parents=True, exist_ok=True)
    log_f = open(run_dir / "stdout.log", "w", encoding="utf-8", buffering=1)

    # ===== [MOD] 更稳的 stdout tee：关闭后别再 flush =====
    class _Tee:
        def __init__(self, *fs):
            self.fs = list(fs)

        def write(self, s: str):
            for f in self.fs:
                try:
                    if hasattr(f, "closed") and f.closed:
                        continue
                    f.write(s)
                    f.flush()
                except Exception:
                    pass

        def flush(self):
            for f in self.fs:
                try:
                    if hasattr(f, "closed") and f.closed:
                        continue
                    f.flush()
                except Exception:
                    pass

    try:
        src = Path(__file__).read_text(encoding="utf-8")
    except Exception as e:
        src = f"<FAILED TO READ __file__: {e}>"
    print("===== SCRIPT SNAPSHOT BEGIN =====", file=log_f)
    print(src, file=log_f)
    print("===== SCRIPT SNAPSHOT END =====", file=log_f)
    log_f.flush()

    sys.stdout = _Tee(sys.__stdout__, log_f)

    def _close_log():
        try:
            log_f.close()
        except Exception:
            pass

    atexit.register(_close_log)
    print(f"[log] run_dir={run_dir.resolve()}")

    torch.set_float32_matmul_precision("high")

    config = ASRConfig(
        train_manifest="dataset/train_fbank_relpath.jsonl",
        dev_manifest="dataset/dev_fbank_relpath.jsonl",
    )

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print("使用设备:", device)

    cmvn_path = Path(config.cmvn_path)
    if cmvn_path.exists():
        cmvn = torch.from_numpy(np.load(cmvn_path)).to(device)
        print(f"已加载 CMVN: {cmvn_path}")
    else:
        cmvn = None
        print(f"警告: CMVN 文件不存在 ({cmvn_path})，跳过归一化")

    print("构建字级词表...")
    tokenizer = CharTokenizer.build_from_jsonl(config.train_manifest)
    print("vocab_size (含 blank):", tokenizer.vocab_size)

    print("构建数据集...")
    train_dataset = JsonlASRDataset(config.train_manifest, tokenizer, config, compute_lengths=True, use_precomputed_fbank=True)
    dev_dataset = JsonlASRDataset(config.dev_manifest, tokenizer, config, compute_lengths=True, use_precomputed_fbank=True)

    print("创建动态 batch 采样器...")
    train_sampler = LengthBucketBatchSampler(
        lengths=train_dataset.lengths,
        batch_size=config.batch_size,
        max_frames_per_batch=config.max_frames_per_batch,
        shuffle=config.shuffle,
        bucket_size=config.bucket_size,
    )
    dev_sampler = LengthBucketBatchSampler(
        lengths=dev_dataset.lengths,
        batch_size=config.batch_size,
        max_frames_per_batch=config.max_frames_per_batch,
        shuffle=False,
        bucket_size=config.bucket_size,
    )

    train_loader = DataLoader(
        train_dataset,
        batch_sampler=train_sampler,
        num_workers=config.num_workers,
        pin_memory=config.pin_memory,
        collate_fn=asr_collate_fn,
        prefetch_factor=config.prefetch_factor,
        persistent_workers=(config.persistent_workers and config.num_workers > 0),
    )
    dev_loader = DataLoader(
        dev_dataset,
        batch_sampler=dev_sampler,
        num_workers=config.num_workers,
        pin_memory=config.pin_memory,
        collate_fn=asr_collate_fn,
        prefetch_factor=config.prefetch_factor,
        persistent_workers=(config.persistent_workers and config.num_workers > 0),
    )

    # ===== [MOD] SpecAugment 放到 main 里实例化（避免全局）=====
    specaug = SpecAugment(
        freq_mask_param=10,
        num_freq_masks=2,
        time_mask_param=50,
        num_time_masks=2,
        protect_last=False,
    )

    model = CTCConformer(
        input_dim=config.num_mel_bins,
        vocab_size=tokenizer.vocab_size,
        encoder_dim=config.encoder_dim,
        ffn_dim=config.ffn_dim,
        num_layers=config.layers,
        num_heads=config.num_heads,
        dropout=config.dropout,
    ).to(device)

    # ===== [MOD] CTC loss：sum + /B（贴近 WeNet CTC 模块）:contentReference[oaicite:9]{index=9} =====
    ctc_loss_fn = nn.CTCLoss(
        blank=tokenizer.blank_id,
        reduction="sum",
        zero_infinity=True,  # inputs too short 时 inf 会被置零 :contentReference[oaicite:10]{index=10}
    )

    optimizer = torch.optim.AdamW(
        model.parameters(),
        lr=config.learning_rate,
        weight_decay=config.weight_decay,
        fused=True,
    )
    lr_sched = WarmupLR(optimizer, warmup_steps=config.warmup_steps)

    best_cer = 1.0
    Path("saved_models").mkdir(parents=True, exist_ok=True)
    best_path = Path(f"saved_models/best_ctc_asr_cer_{time.strftime('%Y%m%d-%H%M%S')}.pt")

    max_T = max(train_dataset.lengths) if train_dataset.lengths else 2048
    min_T = min(train_dataset.lengths) if train_dataset.lengths else 11  # [MOD]
    model = dynamic_pre_compile(train_loader, device, model, max_T=max_T, min_T=min_T)  # [MOD]


    with sdpa_kernel(SDPBackend.MATH):
        for epoch in range(1, config.num_epochs + 1):
            train_one_epoch(
                model=model,
                dataloader=train_loader,
                optimizer=optimizer,
                lr_sched=lr_sched,
                ctc_loss_fn=ctc_loss_fn,
                device=device,
                max_grad_norm=config.max_grad_norm,
                tokenizer=tokenizer,
                cmvn=cmvn,
                specaug=specaug,
                specaug_prob=config.use_specaug_prob + min(epoch * 0.01, 0.4),
                cer_log_interval=config.train_cer_log_interval,
            )

            cer, dev_loss, dbg = evaluate_ctc(
                model=model,
                dataloader=dev_loader,
                tokenizer=tokenizer,
                device=device,
                ctc_loss_fn=ctc_loss_fn,
                cmvn=cmvn,
                max_batches=config.eval_max_batches,
                cer_max_batches=config.eval_cer_max_batches,
            )
            print(
                f"[Epoch {epoch:02d}] dev_loss_per_frame={dev_loss:.4f}, "
                f"dev_CER(sampled)={cer * 100:.2f}% "
                f"[blank_frac={dbg['blank_frac']*100:.2f}%, avg_hyp_len={dbg['avg_hyp_len']:.2f}, invalid_ratio={dbg['invalid_ratio']*100:.2f}%]"
            )

            if cer < best_cer:
                best_cer = cer
                torch.save(
                    {
                        "model_state_dict": model.state_dict(),
                        "config": config.__dict__,
                        "char2id": tokenizer.char2id,
                        "blank_id": tokenizer.blank_id,
                    },
                    best_path,
                )
                print(f"  CER 改善，已保存到 {best_path} (best_CER={best_cer * 100:.2f}%)")

            if device.type == "cuda":
                alloc = torch.cuda.memory_allocated() / 1024**2
                resv = torch.cuda.memory_reserved() / 1024**2
                print(f"allocated={alloc:.1f}MB reserved={resv:.1f}MB lr={optimizer.param_groups[0]['lr']:.3e}")


if __name__ == "__main__":
    main()

===== SCRIPT SNAPSHOT END =====
[log] run_dir=/home/lhc/data/gudsen/asr/logs/20251228-121141
使用设备: cuda
已加载 CMVN: dataset/cmvn.npy
构建字级词表...
vocab_size (含 blank): 4231
构建数据集...
创建动态 batch 采样器...
[compile] ConstraintViolationError, fallback to eager. err=Constraints violated (L['feats'].size()[1])! For more information, run with TORCH_LOGS="+dynamic".
  - Not all values of L['feats'].size()[1] = L['feats'].size()[1] in the specified range 121 <= L['feats'].size()[1] <= 1451 satisfy the generated guard (28 % (28*(((-3) + L['feats'].size()[1]) // 4))) != 0.

train_loss: 69.505 train_cer(sampled): 0.934 invalid_ratio: 0.00%
[Epoch 01] dev_loss_per_frame=27.1893, dev_CER(sampled)=48.69% [blank_frac=89.51%, avg_hyp_len=14.23, invalid_ratio=0.00%]
  CER 改善，已保存到 saved_models/best_ctc_asr_cer_20251228-121142.pt (best_CER=48.69%)
allocated=540.6MB reserved=11306.0MB lr=2.835e-04
train_loss: 22.345 train_cer(sampled): 0.367 invalid_ratio: 0.00%
[Epoch 02] dev_loss_per_frame=14.9029, dev_CER(sampled)=31.14% [blank_frac=89.31%, avg_hyp_len=14.14, invalid_ratio=0.00%]
  CER 改善，已保存到 saved_models/best_ctc_asr_cer_20251228-121142.pt (best_CER=31.14%)
allocated=541.3MB reserved=11306.0MB lr=4.695e-04
train_loss: 13.902 train_cer(sampled): 0.226 invalid_ratio: 0.00%
[Epoch 03] dev_loss_per_frame=10.9270, dev_CER(sampled)=23.29% [blank_frac=89.28%, avg_hyp_len=14.05, invalid_ratio=0.00%]
  CER 改善，已保存到 saved_models/best_ctc_asr_cer_20251228-121142.pt (best_CER=23.29%)
allocated=538.5MB reserved=11306.0MB lr=3.833e-04
train_loss: 10.122 train_cer(sampled): 0.156 invalid_ratio: 0.00%
[Epoch 04] dev_loss_per_frame=8.1664, dev_CER(sampled)=18.72% [blank_frac=89.35%, avg_hyp_len=13.98, invalid_ratio=0.00%]
  CER 改善，已保存到 saved_models/best_ctc_asr_cer_20251228-121142.pt (best_CER=18.72%)
allocated=543.7MB reserved=11306.0MB lr=3.320e-04
train_loss: 8.115 train_cer(sampled): 0.137 invalid_ratio: 0.00%
[Epoch 05] dev_loss_per_frame=7.4797, dev_CER(sampled)=17.70% [blank_frac=88.70%, avg_hyp_len=14.09, invalid_ratio=0.00%]
  CER 改善，已保存到 saved_models/best_ctc_asr_cer_20251228-121142.pt (best_CER=17.70%)
allocated=545.5MB reserved=11306.0MB lr=2.969e-04
train_loss: 6.759 train_cer(sampled): 0.124 invalid_ratio: 0.00%
[Epoch 06] dev_loss_per_frame=6.7905, dev_CER(sampled)=15.76% [blank_frac=88.62%, avg_hyp_len=14.06, invalid_ratio=0.00%]
  CER 改善，已保存到 saved_models/best_ctc_asr_cer_20251228-121142.pt (best_CER=15.76%)
allocated=534.4MB reserved=11306.0MB lr=2.711e-04
train_loss: 5.824 train_cer(sampled): 0.101 invalid_ratio: 0.00%
[Epoch 07] dev_loss_per_frame=6.3419, dev_CER(sampled)=15.37% [blank_frac=89.09%, avg_hyp_len=14.04, invalid_ratio=0.00%]
  CER 改善，已保存到 saved_models/best_ctc_asr_cer_20251228-121142.pt (best_CER=15.37%)
allocated=535.3MB reserved=11306.0MB lr=2.510e-04
train_loss: 5.089 train_cer(sampled): 0.079 invalid_ratio: 0.00%
[Epoch 08] dev_loss_per_frame=6.3072, dev_CER(sampled)=14.61% [blank_frac=88.70%, avg_hyp_len=14.11, invalid_ratio=0.00%]
  CER 改善，已保存到 saved_models/best_ctc_asr_cer_20251228-121142.pt (best_CER=14.61%)
allocated=535.2MB reserved=11306.0MB lr=2.348e-04
train_loss: 4.688 train_cer(sampled): 0.082 invalid_ratio: 0.00%
[Epoch 09] dev_loss_per_frame=6.1943, dev_CER(sampled)=13.89% [blank_frac=88.76%, avg_hyp_len=14.07, invalid_ratio=0.00%]
  CER 改善，已保存到 saved_models/best_ctc_asr_cer_20251228-121142.pt (best_CER=13.89%)
allocated=544.4MB reserved=11306.0MB lr=2.213e-04
train_loss: 4.301 train_cer(sampled): 0.069 invalid_ratio: 0.00%
[Epoch 10] dev_loss_per_frame=6.0363, dev_CER(sampled)=13.23% [blank_frac=88.67%, avg_hyp_len=14.05, invalid_ratio=0.00%]
  CER 改善，已保存到 saved_models/best_ctc_asr_cer_20251228-121142.pt (best_CER=13.23%)
allocated=540.5MB reserved=11306.0MB lr=2.100e-04
train_loss: 3.933 train_cer(sampled): 0.075 invalid_ratio: 0.00%
[Epoch 11] dev_loss_per_frame=5.9399, dev_CER(sampled)=13.14% [blank_frac=88.68%, avg_hyp_len=14.08, invalid_ratio=0.00%]
  CER 改善，已保存到 saved_models/best_ctc_asr_cer_20251228-121142.pt (best_CER=13.14%)
allocated=543.6MB reserved=11306.0MB lr=2.002e-04
train_loss: 3.691 train_cer(sampled): 0.065 invalid_ratio: 0.00%
[Epoch 12] dev_loss_per_frame=5.8391, dev_CER(sampled)=11.91% [blank_frac=88.87%, avg_hyp_len=14.05, invalid_ratio=0.00%]
  CER 改善，已保存到 saved_models/best_ctc_asr_cer_20251228-121142.pt (best_CER=11.91%)
allocated=541.1MB reserved=11306.0MB lr=1.917e-04
train_loss: 3.402 train_cer(sampled): 0.063 invalid_ratio: 0.00%
[Epoch 13] dev_loss_per_frame=5.7962, dev_CER(sampled)=12.45% [blank_frac=88.88%, avg_hyp_len=14.10, invalid_ratio=0.00%]
allocated=534.4MB reserved=11306.0MB lr=1.842e-04
train_loss: 3.213 train_cer(sampled): 0.044 invalid_ratio: 0.00%
[Epoch 14] dev_loss_per_frame=5.8303, dev_CER(sampled)=12.16% [blank_frac=88.87%, avg_hyp_len=14.05, invalid_ratio=0.00%]
allocated=539.9MB reserved=11306.0MB lr=1.775e-04
train_loss: 3.021 train_cer(sampled): 0.052 invalid_ratio: 0.00%
[Epoch 15] dev_loss_per_frame=5.8379, dev_CER(sampled)=11.75% [blank_frac=88.97%, avg_hyp_len=14.09, invalid_ratio=0.00%]
  CER 改善，已保存到 saved_models/best_ctc_asr_cer_20251228-121142.pt (best_CER=11.75%)
allocated=539.5MB reserved=11306.0MB lr=1.714e-04
train_loss: 2.934 train_cer(sampled): 0.055 invalid_ratio: 0.00%
[Epoch 16] dev_loss_per_frame=5.9938, dev_CER(sampled)=12.09% [blank_frac=88.92%, avg_hyp_len=14.07, invalid_ratio=0.00%]
allocated=542.2MB reserved=11306.0MB lr=1.660e-04
train_loss: 2.814 train_cer(sampled): 0.055 invalid_ratio: 0.00%
[Epoch 17] dev_loss_per_frame=6.0182, dev_CER(sampled)=12.16% [blank_frac=89.18%, avg_hyp_len=14.07, invalid_ratio=0.00%]
allocated=543.1MB reserved=11306.0MB lr=1.610e-04
train_loss: 2.796 train_cer(sampled): 0.041 invalid_ratio: 0.00%
[Epoch 18] dev_loss_per_frame=6.0398, dev_CER(sampled)=11.72% [blank_frac=89.10%, avg_hyp_len=14.07, invalid_ratio=0.00%]
  CER 改善，已保存到 saved_models/best_ctc_asr_cer_20251228-121142.pt (best_CER=11.72%)
allocated=536.6MB reserved=11306.0MB lr=1.565e-04
train_loss: 2.700 train_cer(sampled): 0.052 invalid_ratio: 0.00%
[Epoch 19] dev_loss_per_frame=6.0467, dev_CER(sampled)=11.55% [blank_frac=89.06%, avg_hyp_len=14.07, invalid_ratio=0.00%]
  CER 改善，已保存到 saved_models/best_ctc_asr_cer_20251228-121142.pt (best_CER=11.55%)
allocated=549.7MB reserved=11306.0MB lr=1.523e-04
train_loss: 2.650 train_cer(sampled): 0.060 invalid_ratio: 0.00%
[Epoch 20] dev_loss_per_frame=6.0756, dev_CER(sampled)=11.30% [blank_frac=89.07%, avg_hyp_len=14.08, invalid_ratio=0.00%]
  CER 改善，已保存到 saved_models/best_ctc_asr_cer_20251228-121142.pt (best_CER=11.30%)
allocated=535.1MB reserved=11306.0MB lr=1.485e-04
train_loss: 2.574 train_cer(sampled): 0.054 invalid_ratio: 0.00%
[Epoch 21] dev_loss_per_frame=6.1422, dev_CER(sampled)=11.19% [blank_frac=89.04%, avg_hyp_len=14.08, invalid_ratio=0.00%]
  CER 改善，已保存到 saved_models/best_ctc_asr_cer_20251228-121142.pt (best_CER=11.19%)
allocated=545.7MB reserved=11306.0MB lr=1.449e-04
train_loss: 2.503 train_cer(sampled): 0.044 invalid_ratio: 0.00%
[Epoch 22] dev_loss_per_frame=6.1319, dev_CER(sampled)=11.19% [blank_frac=88.99%, avg_hyp_len=14.08, invalid_ratio=0.00%]
allocated=538.8MB reserved=11306.0MB lr=1.416e-04
train_loss: 2.458 train_cer(sampled): 0.029 invalid_ratio: 0.00%
[Epoch 23] dev_loss_per_frame=6.1833, dev_CER(sampled)=11.25% [blank_frac=89.13%, avg_hyp_len=14.09, invalid_ratio=0.00%]
allocated=540.4MB reserved=11306.0MB lr=1.384e-04
train_loss: 2.504 train_cer(sampled): 0.047 invalid_ratio: 0.00%
[Epoch 24] dev_loss_per_frame=6.3085, dev_CER(sampled)=11.29% [blank_frac=89.19%, avg_hyp_len=14.06, invalid_ratio=0.00%]
allocated=542.7MB reserved=11306.0MB lr=1.355e-04
train_loss: 2.549 train_cer(sampled): 0.049 invalid_ratio: 0.00%
[Epoch 25] dev_loss_per_frame=6.3124, dev_CER(sampled)=11.31% [blank_frac=89.07%, avg_hyp_len=14.09, invalid_ratio=0.00%]
allocated=542.2MB reserved=11306.0MB lr=1.328e-04
train_loss: 2.517 train_cer(sampled): 0.052 invalid_ratio: 0.00%
[Epoch 26] dev_loss_per_frame=6.3632, dev_CER(sampled)=11.60% [blank_frac=89.19%, avg_hyp_len=14.08, invalid_ratio=0.00%]
allocated=535.2MB reserved=11306.0MB lr=1.302e-04
train_loss: 2.548 train_cer(sampled): 0.053 invalid_ratio: 0.00%
[Epoch 27] dev_loss_per_frame=6.4504, dev_CER(sampled)=11.05% [blank_frac=89.05%, avg_hyp_len=14.08, invalid_ratio=0.00%]
  CER 改善，已保存到 saved_models/best_ctc_asr_cer_20251228-121142.pt (best_CER=11.05%)
allocated=539.4MB reserved=11306.0MB lr=1.278e-04
train_loss: 2.515 train_cer(sampled): 0.039 invalid_ratio: 0.00%
[Epoch 28] dev_loss_per_frame=6.4133, dev_CER(sampled)=11.06% [blank_frac=89.07%, avg_hyp_len=14.08, invalid_ratio=0.00%]
allocated=541.6MB reserved=11306.0MB lr=1.255e-04
train_loss: 2.499 train_cer(sampled): 0.045 invalid_ratio: 0.00%
[Epoch 29] dev_loss_per_frame=6.4641, dev_CER(sampled)=11.09% [blank_frac=89.06%, avg_hyp_len=14.08, invalid_ratio=0.00%]
allocated=535.1MB reserved=11306.0MB lr=1.233e-04
train_loss: 2.461 train_cer(sampled): 0.039 invalid_ratio: 0.00%
[Epoch 30] dev_loss_per_frame=6.3986, dev_CER(sampled)=10.75% [blank_frac=89.17%, avg_hyp_len=14.08, invalid_ratio=0.00%]
  CER 改善，已保存到 saved_models/best_ctc_asr_cer_20251228-121142.pt (best_CER=10.75%)
allocated=542.3MB reserved=11306.0MB lr=1.212e-04
train_loss: 2.552 train_cer(sampled): 0.032 invalid_ratio: 0.00%
[Epoch 31] dev_loss_per_frame=6.5341, dev_CER(sampled)=10.47% [blank_frac=89.14%, avg_hyp_len=14.08, invalid_ratio=0.00%]
  CER 改善，已保存到 saved_models/best_ctc_asr_cer_20251228-121142.pt (best_CER=10.47%)
allocated=539.4MB reserved=11306.0MB lr=1.193e-04
train_loss: 2.582 train_cer(sampled): 0.037 invalid_ratio: 0.00%
[Epoch 32] dev_loss_per_frame=6.3890, dev_CER(sampled)=10.58% [blank_frac=88.98%, avg_hyp_len=14.07, invalid_ratio=0.00%]
allocated=540.7MB reserved=11306.0MB lr=1.174e-04
train_loss: 2.581 train_cer(sampled): 0.046 invalid_ratio: 0.00%
[Epoch 33] dev_loss_per_frame=6.4552, dev_CER(sampled)=10.72% [blank_frac=89.08%, avg_hyp_len=14.09, invalid_ratio=0.00%]
allocated=545.9MB reserved=11306.0MB lr=1.156e-04
train_loss: 2.583 train_cer(sampled): 0.034 invalid_ratio: 0.00%
[Epoch 34] dev_loss_per_frame=6.5061, dev_CER(sampled)=10.61% [blank_frac=89.02%, avg_hyp_len=14.07, invalid_ratio=0.00%]
allocated=555.8MB reserved=11306.0MB lr=1.139e-04
train_loss: 2.583 train_cer(sampled): 0.049 invalid_ratio: 0.00%
[Epoch 35] dev_loss_per_frame=6.4738, dev_CER(sampled)=10.53% [blank_frac=89.18%, avg_hyp_len=14.09, invalid_ratio=0.00%]
allocated=537.6MB reserved=11306.0MB lr=1.122e-04
train_loss: 2.552 train_cer(sampled): 0.039 invalid_ratio: 0.00%
[Epoch 36] dev_loss_per_frame=6.4810, dev_CER(sampled)=10.49% [blank_frac=89.03%, avg_hyp_len=14.09, invalid_ratio=0.00%]
allocated=548.5MB reserved=11306.0MB lr=1.107e-04
train_loss: 2.611 train_cer(sampled): 0.029 invalid_ratio: 0.00%
[Epoch 37] dev_loss_per_frame=6.5102, dev_CER(sampled)=10.47% [blank_frac=89.03%, avg_hyp_len=14.09, invalid_ratio=0.00%]
allocated=544.2MB reserved=11306.0MB lr=1.092e-04
train_loss: 2.630 train_cer(sampled): 0.037 invalid_ratio: 0.00%
[Epoch 38] dev_loss_per_frame=6.5511, dev_CER(sampled)=10.52% [blank_frac=88.97%, avg_hyp_len=14.08, invalid_ratio=0.00%]
allocated=540.2MB reserved=11306.0MB lr=1.077e-04
train_loss: 2.622 train_cer(sampled): 0.050 invalid_ratio: 0.00%
[Epoch 39] dev_loss_per_frame=6.5453, dev_CER(sampled)=10.30% [blank_frac=89.03%, avg_hyp_len=14.08, invalid_ratio=0.00%]
  CER 改善，已保存到 saved_models/best_ctc_asr_cer_20251228-121142.pt (best_CER=10.30%)
allocated=540.2MB reserved=11306.0MB lr=1.063e-04
train_loss: 2.710 train_cer(sampled): 0.045 invalid_ratio: 0.00%
[Epoch 40] dev_loss_per_frame=6.4958, dev_CER(sampled)=10.43% [blank_frac=89.05%, avg_hyp_len=14.10, invalid_ratio=0.00%]
allocated=541.6MB reserved=11306.0MB lr=1.050e-04
train_loss: 2.624 train_cer(sampled): 0.038 invalid_ratio: 0.00%
[Epoch 41] dev_loss_per_frame=6.4357, dev_CER(sampled)=10.40% [blank_frac=89.12%, avg_hyp_len=14.08, invalid_ratio=0.00%]
allocated=539.8MB reserved=11306.0MB lr=1.037e-04
train_loss: 2.620 train_cer(sampled): 0.048 invalid_ratio: 0.00%
[Epoch 42] dev_loss_per_frame=6.5611, dev_CER(sampled)=10.32% [blank_frac=89.11%, avg_hyp_len=14.09, invalid_ratio=0.00%]
allocated=537.8MB reserved=11306.0MB lr=1.025e-04
train_loss: 2.586 train_cer(sampled): 0.046 invalid_ratio: 0.00%
[Epoch 43] dev_loss_per_frame=6.5747, dev_CER(sampled)=10.69% [blank_frac=89.14%, avg_hyp_len=14.08, invalid_ratio=0.00%]
allocated=533.5MB reserved=11306.0MB lr=1.013e-04
train_loss: 2.529 train_cer(sampled): 0.051 invalid_ratio: 0.00%
[Epoch 44] dev_loss_per_frame=6.5495, dev_CER(sampled)=10.35% [blank_frac=89.08%, avg_hyp_len=14.09, invalid_ratio=0.00%]
allocated=539.4MB reserved=11306.0MB lr=1.001e-04
train_loss: 2.486 train_cer(sampled): 0.059 invalid_ratio: 0.00%
[Epoch 45] dev_loss_per_frame=6.5403, dev_CER(sampled)=10.45% [blank_frac=89.18%, avg_hyp_len=14.09, invalid_ratio=0.00%]
allocated=540.1MB reserved=11306.0MB lr=9.898e-05
train_loss: 2.483 train_cer(sampled): 0.034 invalid_ratio: 0.00%
[Epoch 46] dev_loss_per_frame=6.4776, dev_CER(sampled)=9.87% [blank_frac=89.02%, avg_hyp_len=14.08, invalid_ratio=0.00%]
  CER 改善，已保存到 saved_models/best_ctc_asr_cer_20251228-121142.pt (best_CER=9.87%)
allocated=536.2MB reserved=11306.0MB lr=9.790e-05
train_loss: 2.479 train_cer(sampled): 0.034 invalid_ratio: 0.00%
[Epoch 47] dev_loss_per_frame=6.4951, dev_CER(sampled)=10.19% [blank_frac=89.16%, avg_hyp_len=14.09, invalid_ratio=0.00%]
allocated=544.7MB reserved=11306.0MB lr=9.685e-05
train_loss: 2.459 train_cer(sampled): 0.050 invalid_ratio: 0.00%
[Epoch 48] dev_loss_per_frame=6.5509, dev_CER(sampled)=10.28% [blank_frac=89.09%, avg_hyp_len=14.08, invalid_ratio=0.00%]
allocated=546.8MB reserved=11306.0MB lr=9.584e-05
train_loss: 2.506 train_cer(sampled): 0.034 invalid_ratio: 0.00%
[Epoch 49] dev_loss_per_frame=6.5264, dev_CER(sampled)=10.12% [blank_frac=89.06%, avg_hyp_len=14.08, invalid_ratio=0.00%]
allocated=533.5MB reserved=11306.0MB lr=9.486e-05
train_loss: 2.418 train_cer(sampled): 0.041 invalid_ratio: 0.00%
[Epoch 50] dev_loss_per_frame=6.5398, dev_CER(sampled)=10.26% [blank_frac=89.00%, avg_hyp_len=14.08, invalid_ratio=0.00%]
allocated=541.6MB reserved=11306.0MB lr=9.390e-05
train_loss: 2.384 train_cer(sampled): 0.035 invalid_ratio: 0.00%
[Epoch 51] dev_loss_per_frame=6.5811, dev_CER(sampled)=10.17% [blank_frac=88.75%, avg_hyp_len=14.09, invalid_ratio=0.00%]
allocated=538.0MB reserved=11306.0MB lr=9.298e-05
train_loss: 2.391 train_cer(sampled): 0.039 invalid_ratio: 0.00%
[Epoch 52] dev_loss_per_frame=6.4984, dev_CER(sampled)=10.25% [blank_frac=88.98%, avg_hyp_len=14.07, invalid_ratio=0.00%]
allocated=540.3MB reserved=11306.0MB lr=9.208e-05
train_loss: 2.392 train_cer(sampled): 0.034 invalid_ratio: 0.00%
[Epoch 53] dev_loss_per_frame=6.5419, dev_CER(sampled)=10.49% [blank_frac=89.04%, avg_hyp_len=14.08, invalid_ratio=0.00%]
allocated=542.5MB reserved=11306.0MB lr=9.121e-05
train_loss: 2.358 train_cer(sampled): 0.045 invalid_ratio: 0.00%
[Epoch 54] dev_loss_per_frame=6.4746, dev_CER(sampled)=10.17% [blank_frac=89.12%, avg_hyp_len=14.08, invalid_ratio=0.00%]
allocated=543.0MB reserved=11306.0MB lr=9.036e-05
train_loss: 2.360 train_cer(sampled): 0.033 invalid_ratio: 0.00%
[Epoch 55] dev_loss_per_frame=6.5708, dev_CER(sampled)=10.30% [blank_frac=88.88%, avg_hyp_len=14.08, invalid_ratio=0.00%]
allocated=542.1MB reserved=11306.0MB lr=8.953e-05
train_loss: 2.401 train_cer(sampled): 0.035 invalid_ratio: 0.00%
[Epoch 56] dev_loss_per_frame=6.6016, dev_CER(sampled)=9.84% [blank_frac=88.97%, avg_hyp_len=14.08, invalid_ratio=0.00%]
  CER 改善，已保存到 saved_models/best_ctc_asr_cer_20251228-121142.pt (best_CER=9.84%)
allocated=546.4MB reserved=11306.0MB lr=8.873e-05
train_loss: 2.329 train_cer(sampled): 0.045 invalid_ratio: 0.00%
[Epoch 57] dev_loss_per_frame=6.5829, dev_CER(sampled)=9.98% [blank_frac=88.93%, avg_hyp_len=14.07, invalid_ratio=0.00%]
allocated=544.6MB reserved=11306.0MB lr=8.795e-05
train_loss: 2.248 train_cer(sampled): 0.032 invalid_ratio: 0.00%
[Epoch 58] dev_loss_per_frame=6.5739, dev_CER(sampled)=10.01% [blank_frac=89.05%, avg_hyp_len=14.07, invalid_ratio=0.00%]
allocated=536.6MB reserved=11306.0MB lr=8.719e-05
train_loss: 2.339 train_cer(sampled): 0.035 invalid_ratio: 0.00%
[Epoch 59] dev_loss_per_frame=6.5674, dev_CER(sampled)=9.98% [blank_frac=89.03%, avg_hyp_len=14.08, invalid_ratio=0.00%]
allocated=536.1MB reserved=11306.0MB lr=8.644e-05
train_loss: 2.272 train_cer(sampled): 0.032 invalid_ratio: 0.00%
[Epoch 60] dev_loss_per_frame=6.5621, dev_CER(sampled)=10.27% [blank_frac=89.00%, avg_hyp_len=14.07, invalid_ratio=0.00%]
allocated=540.6MB reserved=11306.0MB lr=8.572e-05
train_loss: 2.235 train_cer(sampled): 0.035 invalid_ratio: 0.00%
[Epoch 61] dev_loss_per_frame=6.5110, dev_CER(sampled)=10.04% [blank_frac=88.95%, avg_hyp_len=14.09, invalid_ratio=0.00%]
allocated=539.8MB reserved=11306.0MB lr=8.502e-05
train_loss: 2.271 train_cer(sampled): 0.022 invalid_ratio: 0.00%
[Epoch 62] dev_loss_per_frame=6.5508, dev_CER(sampled)=9.60% [blank_frac=88.97%, avg_hyp_len=14.09, invalid_ratio=0.00%]
  CER 改善，已保存到 saved_models/best_ctc_asr_cer_20251228-121142.pt (best_CER=9.60%)
allocated=538.1MB reserved=11306.0MB lr=8.433e-05
train_loss: 2.221 train_cer(sampled): 0.029 invalid_ratio: 0.00%
[Epoch 63] dev_loss_per_frame=6.5679, dev_CER(sampled)=10.01% [blank_frac=89.12%, avg_hyp_len=14.07, invalid_ratio=0.00%]
allocated=535.0MB reserved=11306.0MB lr=8.365e-05
train_loss: 2.236 train_cer(sampled): 0.035 invalid_ratio: 0.00%
[Epoch 64] dev_loss_per_frame=6.5252, dev_CER(sampled)=9.95% [blank_frac=89.02%, avg_hyp_len=14.07, invalid_ratio=0.00%]
allocated=538.7MB reserved=11306.0MB lr=8.300e-05
