===== SCRIPT SNAPSHOT BEGIN =====
#!/usr/bin/env python
# -*- coding: utf-8 -*-
# @Time    : 2025/11/22
# @Author  : Joyful Buffalo
# @File    : simple_ctc_asr.py
# !/usr/bin/env python
# -*- coding: utf-8 -*-
from __future__ import annotations

import json
import math
import random
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Dict, List, Tuple, Optional, Iterator, cast

import numpy as np
import torch
import torchaudio
from torch import nn
from torch.nn import functional as F
from torch.nn.attention import sdpa_kernel, SDPBackend
from torch.nn.utils.rnn import pad_sequence
from torch.utils.data import Dataset, DataLoader, Sampler
from torchaudio.compliance import kaldi
from tqdm import tqdm
from utils.specAug import SpecAugment


@dataclass
class ASRConfig:
    train_manifest: str
    dev_manifest: str
    cmvn_path: str = "dataset/cmvn.npy"
    sample_rate: int = 16000
    num_mel_bins: int = 80
    frame_length_ms: float = 25.0
    frame_shift_ms: float = 10.0
    dither: float = 0.0
    hidden_size: int = 256
    layers: int = 2
    dropout: float = 0.1
    batch_size: Optional[int] = None
    max_frames_per_batch: Optional[int] = 8192
    num_epochs: int = 10
    learning_rate: float = 1e-3
    weight_decay: float = 1e-5
    max_grad_norm: float = 5.0
    num_workers: int = 4
    prefetch_factor: int = 1
    bucket_size: int = 100
    pin_memory: bool = True
    shuffle: bool = True


# ======================
# 动态batch采样器
# ======================
class LengthBucketBatchSampler(Sampler[List[int]]):
    """按长度分桶的动态batch采样器，支持固定batch_size或max_frames_per_batch"""

    def __init__(
            self,
            lengths: List[int],
            batch_size: Optional[int] = None,
            max_frames_per_batch: Optional[int] = None,
            shuffle: bool = True,
            bucket_size: int = 100,
    ):
        super(LengthBucketBatchSampler, self).__init__(None)
        assert (batch_size is None) ^ (max_frames_per_batch is None), \
            "Exactly one of batch_size or max_frames_per_batch should be provided."
        self.lengths = lengths
        self.batch_size = batch_size
        self.max_frames_per_batch = max_frames_per_batch
        self.shuffle = shuffle
        self.bucket_size = bucket_size
        self._indices = list(range(len(lengths)))
        self.g = torch.Generator().manual_seed(torch.seed())

    def __iter__(self) -> Iterator[List[int]]:
        # 返回 batch indices
        if self.shuffle:
            indices = [self._indices[i] for i in torch.randperm(len(self._indices), generator=self.g).tolist()]
        else:
            indices = self._indices

        # 分桶 + 桶内按长度升序
        buckets = [
            indices[i:i + self.bucket_size]
            for i in range(0, len(indices), self.bucket_size)
        ]
        for b in buckets:
            b.sort(key=lambda i: self.lengths[i])

        flat = [i for b in buckets for i in b]

        if self.batch_size is not None:
            for i in range(0, len(flat), self.batch_size):
                yield flat[i:i + self.batch_size]
        else:
            current, cur_frames = [], 0
            for i in flat:
                length = int(self.lengths[i])
                if len(current) > 0 and (cur_frames + length) > int(self.max_frames_per_batch):
                    yield current
                    current, cur_frames = [], 0
                current.append(i)
                cur_frames += length
            if len(current) > 0:
                yield current

    def __len__(self) -> int:
        if self.batch_size is not None:
            return math.ceil(len(self.lengths) / self.batch_size)
        # max_frames 模式下，__len__ 很难精确；给出近似上界
        avg = (sum(self.lengths) / max(1, len(self.lengths)))
        return max(1, int(sum(self.lengths) / max(self.max_frames_per_batch, cast(int, avg))))


# ======================
# 字级 tokenizer（0 留给 CTC blank）
# ======================
class CharTokenizer:
    """最简单的“字级”分词器，0 号留给 CTC blank。"""

    def __init__(self, char2id: Dict[str, int]) -> None:
        self.blank_id: int = 0
        self.char2id: Dict[str, int] = char2id
        # id2char 索引 0 为 <blank>，其余按 id 排序
        self.id2char: List[str] = ["<blank>"] + [
            ch for ch, _ in sorted(char2id.items(), key=lambda x: x[1])
        ]

    @classmethod
    def build_from_jsonl(cls, manifest_path: str) -> "CharTokenizer":
        chars: set[str] = set()
        path = Path(manifest_path)
        with path.open("r", encoding="utf-8") as f:
            for line in f:
                if not line.strip():
                    continue
                item = json.loads(line)
                # 支持 txt 或 text 两种字段名
                txt = item.get("txt") or item.get("text")
                if txt is None:
                    raise ValueError("每行 json 需要包含 'txt' 或 'text' 字段")
                for ch in txt:
                    if ch.isspace():
                        continue  # 去掉空格
                    chars.add(ch)
        # 按字典序排序，id 从 1 开始分配，0 保留给 blank
        char2id: Dict[str, int] = {}
        next_id = 1
        for ch in sorted(chars):
            char2id[ch] = next_id
            next_id += 1
        return cls(char2id)

    @property
    def vocab_size(self) -> int:
        # 包含 blank
        return 1 + len(self.char2id)

    def encode(self, text: str) -> List[int]:
        ids: List[int] = []
        for ch in text:
            if ch.isspace():
                continue
            idx = self.char2id.get(ch)
            if idx is None:
                # 简单处理：丢弃未登录字（也可以映射到 <unk>，这里先偷懒）
                continue
            ids.append(idx)
        return ids

    def decode_ids(self, ids: List[int]) -> str:
        chars: List[str] = []
        for idx in ids:
            if idx == self.blank_id:
                continue
            if 0 <= idx < len(self.id2char):
                ch = self.id2char[idx]
                if ch not in ("<blank>", "<unk>"):
                    chars.append(ch)
        return "".join(chars)


class JsonlASRDataset(Dataset):
    """读取 jsonl 格式清单：{"key":..., "wav":..., "txt":...} 或 {"key":..., "feat":..., "txt":..., "feat_frames":...}"""

    def __init__(self, manifest_path: str, tokenizer: CharTokenizer, config: ASRConfig,
                 compute_lengths: bool = False, use_precomputed_fbank: bool = False) -> None:
        super().__init__()
        self.manifest_path = Path(manifest_path)
        self.tokenizer = tokenizer
        self.sample_rate = config.sample_rate
        self.num_mel_bins = config.num_mel_bins
        self.frame_length_ms = config.frame_length_ms
        self.frame_shift_ms = config.frame_shift_ms
        self.dither = config.dither
        self.compute_lengths = compute_lengths
        self.use_precomputed_fbank = use_precomputed_fbank
        # self.aug = SpecAugment(
        #     freq_mask_param=10,
        #     num_freq_masks=2,
        #     time_mask_param=50,
        #     num_time_masks=2,
        #     protect_last=False,
        # )

        self.entries: List[Dict[str, Any]] = []
        self.lengths: List[int] = []

        with self.manifest_path.open("r", encoding="utf-8") as f:
            for line in f:
                if not line.strip():
                    continue
                item = json.loads(line)
                txt = item.get("txt") or item.get("text")
                if txt is None:
                    raise ValueError("清单每行必须包含 'txt'/'text'")

                # 判断是使用预处理的fbank还是原始wav
                if use_precomputed_fbank:
                    feat = item.get("feat")
                    # if random.random()< 0.5:
                    #     feat = self.aug(feat)
                    if feat is None:
                        raise ValueError("使用预处理fbank时，清单每行必须包含 'feat'")
                    self.entries.append({"feat": feat, "txt": txt})

                    # 如果需要计算长度（用于动态batch）
                    if compute_lengths:
                        feat_frames = item.get("feat_frames", 0)
                        if feat_frames > 0:
                            self.lengths.append(feat_frames)
                        else:
                            print(f"警告: {feat} 没有帧数信息，使用默认值")
                            self.lengths.append(100)
                else:
                    wav = item.get("wav")
                    if wav is None:
                        raise ValueError("清单每行必须包含 'wav'")
                    self.entries.append({"wav": wav, "txt": txt})

                    # 如果需要计算长度（用于动态batch）
                    if compute_lengths:
                        # 从JSON中读取时长（秒），计算特征帧数
                        duration_sec = item.get("length", 0.0)
                        if duration_sec > 0:
                            duration_ms = duration_sec * 1000
                            feat_frames = int((duration_ms - self.frame_length_ms) / self.frame_shift_ms) + 1
                            self.lengths.append(max(1, feat_frames))
                        else:
                            # 如果没有时长信息，给一个默认值
                            print(f"警告: {wav} 没有时长信息，使用默认值")
                            self.lengths.append(100)

        if not self.entries:
            raise RuntimeError(f"{self.manifest_path} 为空")

    def __len__(self) -> int:
        return len(self.entries)

    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor, str]:
        item = self.entries[idx]
        text: str = item["txt"]

        if self.use_precomputed_fbank:
            # 使用预处理好的fbank特征
            feat_path = item["feat"]
            # 使用 numpy 加载更快（特别是在 WSL2 中）
            if feat_path.endswith('.npy'):
                feat = torch.from_numpy(np.load(feat_path))
            else:
                # 兼容 .pt 格式，使用 weights_only 更快更安全
                feat = torch.load(feat_path, weights_only=False)  # (frames, num_mel_bins)
        else:
            # 实时计算fbank特征
            wav_path = item["wav"]
            waveform, sr = torchaudio.load(wav_path)  # (channels, num_samples)
            if waveform.size(0) > 1:
                waveform = waveform.mean(dim=0, keepdim=True)

            if sr != self.sample_rate:
                resampler = torchaudio.transforms.Resample(orig_freq=sr, new_freq=self.sample_rate)
                waveform = resampler(waveform)
                sr = self.sample_rate

            # 4) 与 Kaldi 一致，将 [-1, 1] 量化到 16bit 整数范围
            #    这个写法直接参考 WeNet 官方的 compute_fbank 实现:contentReference[oaicite:5]{index=5}
            waveform = waveform * (1 << 15)

            # 5) 计算 fbank（API 见 torchaudio.compliance.kaldi.fbank 官方文档:contentReference[oaicite:6]{index=6}）
            feat = kaldi.fbank(
                waveform,
                num_mel_bins=self.num_mel_bins,
                frame_length=self.frame_length_ms,
                frame_shift=self.frame_shift_ms,
                dither=self.dither,
                energy_floor=0.0,
                sample_frequency=float(sr),
            )  # (frames, num_mel_bins)

        # 文本转为 id 序列
        target_ids = torch.tensor(self.tokenizer.encode(text), dtype=torch.long)
        return feat, target_ids, text


def asr_collate_fn(
        batch: List[Tuple[torch.Tensor, torch.Tensor, str]]
) -> Dict[str, Any]:
    feats, targets, texts = zip(*batch)

    # 对 fbank 按时间维做 padding
    feat_lengths = torch.tensor([f.size(0) for f in feats], dtype=torch.long)
    padded_feats = pad_sequence(feats, batch_first=True)  # (B, T_max, D)

    # 标签拼接成一维向量 + 长度
    target_lengths = torch.tensor([t.size(0) for t in targets], dtype=torch.long)
    concatenated_targets = torch.cat(targets, dim=0)  # (sum_target,)

    return {
        "feats": padded_feats,
        "feat_lengths": feat_lengths,
        "max_feat_lengths": feat_lengths.max().item(),
        "targets": concatenated_targets,
        "target_lengths": target_lengths,
        "texts": list(texts),
    }


class CTCConformer(nn.Module):
    def __init__(
            self,
            input_dim: int,
            vocab_size: int,
            hidden_size: int = 512,
            num_layers: int = 6,
            dropout: float = 0.1,
    ) -> None:
        super().__init__()
        self.input_proj = nn.Sequential(
            nn.Linear(input_dim, input_dim * 4),
            nn.ReLU(inplace=True),
            nn.Linear(input_dim * 4, input_dim * 2)
        )
        
        self.conformer = torchaudio.models.Conformer(
            input_dim=input_dim * 2,
            num_layers=num_layers,
            ffn_dim=hidden_size,
            dropout=dropout,
            depthwise_conv_kernel_size=15,
            num_heads=8
        )
        self.output_proj = nn.Sequential(
            nn.Linear(input_dim * 2, vocab_size),
        )

    def forward(self, feats: torch.Tensor, feat_lengths, max_length) -> torch.Tensor:
        feats = self.input_proj(feats)
        x, _ = self.conformer(feats, feat_lengths)  # (B, T, 2H)
        logits = self.output_proj(x)  # (B, T, V)
        return logits


def ctc_greedy_decode(
        logit_batch: torch.Tensor,
        feat_lengths: torch.Tensor,
        blank_id: int,
) -> List[List[int]]:
    """
    最简单 CTC 贪心解码：按时间取 argmax，然后压缩重复 & 去掉 blank。
    logit_batch: (B, T, V)
    feat_lengths: (B,)
    """
    with torch.no_grad():
        probs = F.log_softmax(logit_batch, dim=-1)
        pred_ids = torch.argmax(probs, dim=-1)  # (B, T)

    results: List[List[int]] = []
    for b in range(pred_ids.size(0)):
        length = int(feat_lengths[b].item())
        seq = pred_ids[b, :length].tolist()
        collapsed: List[int] = []
        prev = blank_id
        for idx in seq:
            if idx != blank_id and idx != prev:
                collapsed.append(idx)
            prev = idx
        results.append(collapsed)
    return results


def char_edit_distance(ref: str, hyp: str) -> int:
    """标准 Levenshtein 编辑距离（以“字”为单位）"""
    ref_chars = list(ref)
    hyp_chars = list(hyp)
    n = len(ref_chars)
    m = len(hyp_chars)
    if n == 0:
        return m
    if m == 0:
        return n
    # DP 矩阵大小 (n+1) x (m+1)
    dp = [[0] * (m + 1) for _ in range(n + 1)]
    for i in range(n + 1):
        dp[i][0] = i
    for j in range(m + 1):
        dp[0][j] = j
    for i in range(1, n + 1):
        for j in range(1, m + 1):
            cost = 0 if ref_chars[i - 1] == hyp_chars[j - 1] else 1
            dp[i][j] = min(
                dp[i - 1][j] + 1,  # 删除
                dp[i][j - 1] + 1,  # 插入
                dp[i - 1][j - 1] + cost,  # 替换
            )
    return dp[n][m]


def evaluate_cer(
        model: nn.Module,
        dataloader: DataLoader,
        tokenizer: CharTokenizer,
        device: torch.device,
        ctc_loss: nn.CTCLoss,
        cmvn: Optional[torch.Tensor] = None,
) -> tuple[float, float]:
    model.eval()
    total_edit = 0
    total_chars = 0
    total_loss = 0
    total_frame = 0
    with torch.no_grad():
        for batch in tqdm(dataloader, ncols=100):
            feats = batch["feats"].to(device)
            feat_lengths = batch["feat_lengths"].to(device)
            texts = batch["texts"]
            targets = batch["targets"].to(device, non_blocking=True)
            max_length = batch["max_feat_lengths"]
            target_lengths = batch["target_lengths"].to(device, non_blocking=True)
            # CMVN 归一化
            if cmvn is not None:
                feats = (feats - cmvn[0]) / cmvn[1]
            logits = model(feats, feat_lengths, max_length)  # (B, T, V)
            log_probs = F.log_softmax(logits, dim=-1).transpose(0, 1)
            loss = ctc_loss(log_probs, targets, feat_lengths, target_lengths)
            pred_id_seqs = ctc_greedy_decode(logits, feat_lengths, tokenizer.blank_id)
            batch_frame = int(feat_lengths.sum().item())
            total_loss += loss.item() * batch_frame
            total_frame += batch_frame
            for ref_text, pred_ids in zip(texts, pred_id_seqs):
                hyp_text = tokenizer.decode_ids(pred_ids)
                # 这里也去掉 ref 中的空白字符
                ref = "".join(ch for ch in ref_text if not ch.isspace())
                edit = char_edit_distance(ref, hyp_text)
                total_edit += edit
                total_chars += len(ref)
    if total_chars == 0:
        return 0.0, 0.
    return total_edit / total_chars, total_loss / total_frame


aug = SpecAugment(
    freq_mask_param=10,  # F = 10
    num_freq_masks=2,  # mF = 2
    time_mask_param=50,  # T = 50（帧）
    num_time_masks=2,  # mT = 2
    protect_last=False,
)


def train_one_epoch(
        model: nn.Module,
        dataloader: DataLoader,
        optimizer: torch.optim.Optimizer,
        ctc_loss: nn.CTCLoss,
        device: torch.device,
        max_grad_norm: float,
        tokenizer: CharTokenizer,
        cmvn: Optional[torch.Tensor] = None,
) -> float:
    model.train()
    total_loss = 0.0
    total_frames = 0
    total_edit = 0
    total_chars = 0
    idx = 0
    for batch in tqdm(dataloader, ncols=100, position=1):
        feats = batch["feats"].to(device, non_blocking=True)
        feat_lengths = batch["feat_lengths"].to(device, non_blocking=True)
        targets = batch["targets"].to(device, non_blocking=True)
        target_lengths = batch["target_lengths"].to(device, non_blocking=True)
        max_length = batch["max_feat_lengths"]
        # CMVN 归一化
        if cmvn is not None:
            feats = (feats - cmvn[0]) / cmvn[1]
        if random.random() < 0.1:
            feats = aug(feats)

        optimizer.zero_grad(set_to_none=True)
        with torch.autocast(device_type='cuda', dtype=torch.bfloat16):
            logits = model(feats, feat_lengths, max_length)  # (B, T, V)
            # CTCLoss 官方文档要求 log_probs 形状为 (T, N, C):contentReference[oaicite:7]{index=7}
            log_probs = F.log_softmax(logits, dim=-1).transpose(0, 1)

        loss = ctc_loss(log_probs, targets, feat_lengths, target_lengths)
        loss.backward()
        if max_grad_norm > 0.0:
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)
        optimizer.step()

        batch_frames = int(feat_lengths.sum().item())
        total_loss += loss.item() * batch_frames
        total_frames += batch_frames
        if idx % 100 == 0:
            with torch.no_grad():
                pred_id_seqs = ctc_greedy_decode(logits, feat_lengths, tokenizer.blank_id)
                offset = 0
                for b, pred_ids in enumerate(pred_id_seqs):
                    cur_len = int(target_lengths[b].item())
                    tgt_ids = targets[offset:offset + cur_len].tolist()
                    offset += cur_len
                    ref_text = tokenizer.decode_ids(tgt_ids)
                    hyp_text = tokenizer.decode_ids(pred_ids)
                    edit = char_edit_distance(ref_text, hyp_text)
                    total_edit += edit
                    total_chars += len(ref_text)
        idx += 1
    print(f'train_loss: {total_loss / total_frames:.3f} train cer: {total_edit / total_chars:.3f}')
    if total_frames == 0:
        return 0.0
    return total_loss / total_frames


def dynamic_pre_compile(train_loader, device, model, max_T):
    warmup_batch = next(iter(train_loader))
    warmup_feats = warmup_batch["feats"].to(device)  # (B, T, D)
    warmup_feat_lengths = warmup_batch["feat_lengths"].to(device)
    warmup_max_len = int(warmup_feat_lengths.max().item())

    torch._dynamo.mark_dynamic(warmup_feats, 1, min=1, max=int(max_T))
    model = torch.compile(model)
    with torch.no_grad():
        _ = model(warmup_feats, warmup_feat_lengths, warmup_max_len)
    return model


def main() -> None:
    import time, sys, atexit
    run_dir = Path("logs") / time.strftime("%Y%m%d-%H%M%S")
    run_dir.mkdir(parents=True, exist_ok=True)
    log_f = open(run_dir / "stdout.log", "w", encoding="utf-8", buffering=1)
    try:
        src = Path(__file__).read_text(encoding="utf-8")
    except Exception as e:
        src = f"<FAILED TO READ __file__: {e}>"
    print("===== SCRIPT SNAPSHOT BEGIN =====", file=log_f)
    print(src, file=log_f)
    print("===== SCRIPT SNAPSHOT END =====", file=log_f)
    log_f.flush()
    class _Tee:
        def __init__(self, *fs): self.fs = fs
        def write(self, s):
            for f in self.fs:
                f.write(s); f.flush()
        def flush(self):
            for f in self.fs:
                f.flush()
    sys.stdout = _Tee(sys.__stdout__, log_f)
    atexit.register(log_f.close)
    print(f"[log] run_dir={run_dir.resolve()}")

    torch.set_float32_matmul_precision('high')
    config = ASRConfig(
        train_manifest="dataset/train_fbank_relpath.jsonl",
        dev_manifest="dataset/dev_fbank_relpath.jsonl",
    )

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print("使用设备:", device)

    # === 加载 CMVN ===
    cmvn_path = Path(config.cmvn_path)
    if cmvn_path.exists():
        cmvn = torch.from_numpy(np.load(cmvn_path)).to(device)  # (2, num_mel_bins)
        print(f"已加载 CMVN: {cmvn_path}")
    else:
        cmvn = None
        print(f"警告: CMVN 文件不存在 ({cmvn_path})，跳过归一化")

    # === 1. 根据训练集构建"字级"词表 ===
    print("构建字级词表...")
    tokenizer = CharTokenizer.build_from_jsonl(config.train_manifest)
    print("vocab_size (含 blank):", tokenizer.vocab_size)

    # === 2. 构建 Dataset / DataLoader ===
    print("构建数据集...")
    train_dataset = JsonlASRDataset(config.train_manifest, tokenizer, config, compute_lengths=True, use_precomputed_fbank=True)
    dev_dataset = JsonlASRDataset(config.dev_manifest, tokenizer, config, compute_lengths=True, use_precomputed_fbank=True)

    # 创建动态batch采样器
    print("创建动态batch采样器...")
    train_sampler = LengthBucketBatchSampler(
        lengths=train_dataset.lengths,
        batch_size=config.batch_size,
        max_frames_per_batch=config.max_frames_per_batch,
        shuffle=config.shuffle,
        bucket_size=config.bucket_size,
    )
    dev_sampler = LengthBucketBatchSampler(
        lengths=dev_dataset.lengths,
        batch_size=config.batch_size,
        max_frames_per_batch=config.max_frames_per_batch,
        shuffle=False,
        bucket_size=config.bucket_size,
    )

    train_loader = DataLoader(
        train_dataset,
        batch_sampler=train_sampler,
        num_workers=config.num_workers,
        pin_memory=config.pin_memory,
        collate_fn=asr_collate_fn,
        prefetch_factor=config.prefetch_factor,
    )
    dev_loader = DataLoader(
        dev_dataset,
        batch_sampler=dev_sampler,
        num_workers=config.num_workers,
        pin_memory=config.pin_memory,
        collate_fn=asr_collate_fn,
        prefetch_factor=config.prefetch_factor,
    )
    # === 3. 构建模型 / 损失 / 优化器 ===
    input_dim = config.num_mel_bins
    vocab_size = tokenizer.vocab_size
    model = CTCConformer(
        input_dim=input_dim,
        vocab_size=vocab_size,
        hidden_size=config.hidden_size,
        num_layers=config.layers,
        dropout=config.dropout,
    ).to(device)

    ctc_loss = nn.CTCLoss(
        blank=tokenizer.blank_id,
        reduction="mean",
        zero_infinity=True,
    )
    optimizer = torch.optim.AdamW(
        model.parameters(),
        lr=config.learning_rate,
        weight_decay=config.weight_decay,
        fused=True
    )
    lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
        optimizer,
        mode="min",
        patience=3,
        factor=0.5,
    )
    best_cer = 1.0
    best_path = Path(f"saved_models/best_ctc_asr_cer_{time.strftime("%Y%m%d-%H%M%S")}.pt")

    epoch = 1

    max_T = max(train_dataset.lengths) if train_dataset.lengths else 2048
    model = dynamic_pre_compile(train_loader, device, model, max_T=max_T)
    with sdpa_kernel(SDPBackend.MATH):
        while True:
            train_loss = train_one_epoch(
                model,
                train_loader,
                optimizer,
                ctc_loss,
                device,
                config.max_grad_norm,
                tokenizer,
                cmvn,
            )
            cer, loss = evaluate_cer(model, dev_loader, tokenizer, device, ctc_loss, cmvn)
            print(
                f"[Epoch {epoch:02d}] dev_loss_per_frame={loss:.4f}, "
                f"dev_CER={cer * 100:.2f}%"
            )
            old_lr = optimizer.param_groups[0]["lr"]
            lr_scheduler.step(loss)
            new_lr = optimizer.param_groups[0]["lr"]
            if new_lr < old_lr:
                print(f'old_lr={old_lr}, new_lr={new_lr}')
            if cer < best_cer:
                best_cer = cer
                torch.save(
                    {
                        "model_state_dict": model.state_dict(),
                        "config": config.__dict__,
                        "char2id": tokenizer.char2id,
                        "blank_id": tokenizer.blank_id,
                    },
                    best_path,
                )
                print(f"  CER 改善，已保存到 {best_path} (best_CER={best_cer * 100:.2f}%)")
            # torch.cuda.empty_cache()
            alloc = torch.cuda.memory_allocated() / 1024**2
            resv  = torch.cuda.memory_reserved()  / 1024**2
            print(f"allocated={alloc:.1f}MB reserved={resv:.1f}MB")

            epoch += 1


if __name__ == "__main__":
    main()

===== SCRIPT SNAPSHOT END =====
[log] run_dir=/home/lhc/data/gudsen/asr/logs/20251227-201827
使用设备: cuda
已加载 CMVN: dataset/cmvn.npy
构建字级词表...
vocab_size (含 blank): 4231
构建数据集...
创建动态batch采样器...
train_loss: 3.358 train cer: 1.049
[Epoch 01] dev_loss_per_frame=1.7805, dev_CER=46.09%
  CER 改善，已保存到 saved_models/best_ctc_asr_cer_20251227-201828.pt (best_CER=46.09%)
allocated=39.2MB reserved=14162.0MB
train_loss: 1.824 train cer: 0.474
[Epoch 02] dev_loss_per_frame=1.4717, dev_CER=39.39%
  CER 改善，已保存到 saved_models/best_ctc_asr_cer_20251227-201828.pt (best_CER=39.39%)
allocated=39.2MB reserved=8514.0MB
train_loss: 1.623 train cer: 0.429
[Epoch 03] dev_loss_per_frame=1.3852, dev_CER=37.07%
  CER 改善，已保存到 saved_models/best_ctc_asr_cer_20251227-201828.pt (best_CER=37.07%)
allocated=39.4MB reserved=8514.0MB
train_loss: 1.524 train cer: 0.406
[Epoch 04] dev_loss_per_frame=1.3019, dev_CER=34.90%
  CER 改善，已保存到 saved_models/best_ctc_asr_cer_20251227-201828.pt (best_CER=34.90%)
allocated=39.2MB reserved=8514.0MB
train_loss: 1.439 train cer: 0.391
[Epoch 05] dev_loss_per_frame=1.2910, dev_CER=34.81%
  CER 改善，已保存到 saved_models/best_ctc_asr_cer_20251227-201828.pt (best_CER=34.81%)
allocated=39.5MB reserved=8522.0MB
train_loss: 1.386 train cer: 0.371
[Epoch 06] dev_loss_per_frame=1.2233, dev_CER=33.04%
  CER 改善，已保存到 saved_models/best_ctc_asr_cer_20251227-201828.pt (best_CER=33.04%)
allocated=39.9MB reserved=8522.0MB
train_loss: 1.326 train cer: 0.358
[Epoch 07] dev_loss_per_frame=1.2015, dev_CER=32.57%
  CER 改善，已保存到 saved_models/best_ctc_asr_cer_20251227-201828.pt (best_CER=32.57%)
allocated=40.0MB reserved=8522.0MB
train_loss: 1.281 train cer: 0.347
[Epoch 08] dev_loss_per_frame=1.1827, dev_CER=31.60%
  CER 改善，已保存到 saved_models/best_ctc_asr_cer_20251227-201828.pt (best_CER=31.60%)
allocated=39.2MB reserved=8522.0MB
train_loss: 1.261 train cer: 0.344
[Epoch 09] dev_loss_per_frame=1.1160, dev_CER=30.22%
  CER 改善，已保存到 saved_models/best_ctc_asr_cer_20251227-201828.pt (best_CER=30.22%)
allocated=39.2MB reserved=8522.0MB
train_loss: 1.218 train cer: 0.339
[Epoch 10] dev_loss_per_frame=1.1150, dev_CER=30.15%
  CER 改善，已保存到 saved_models/best_ctc_asr_cer_20251227-201828.pt (best_CER=30.15%)
allocated=39.6MB reserved=8522.0MB
train_loss: 1.219 train cer: 0.338
[Epoch 11] dev_loss_per_frame=1.1190, dev_CER=30.29%
allocated=39.5MB reserved=8526.0MB
train_loss: 1.186 train cer: 0.329
[Epoch 12] dev_loss_per_frame=1.0687, dev_CER=29.16%
  CER 改善，已保存到 saved_models/best_ctc_asr_cer_20251227-201828.pt (best_CER=29.16%)
allocated=39.2MB reserved=8526.0MB
train_loss: 1.161 train cer: 0.328
[Epoch 13] dev_loss_per_frame=1.0419, dev_CER=28.33%
  CER 改善，已保存到 saved_models/best_ctc_asr_cer_20251227-201828.pt (best_CER=28.33%)
allocated=39.5MB reserved=8526.0MB
train_loss: 1.130 train cer: 0.304
[Epoch 14] dev_loss_per_frame=1.0316, dev_CER=27.71%
  CER 改善，已保存到 saved_models/best_ctc_asr_cer_20251227-201828.pt (best_CER=27.71%)
allocated=39.4MB reserved=8526.0MB
train_loss: 1.143 train cer: 0.313
[Epoch 15] dev_loss_per_frame=1.0338, dev_CER=27.81%
allocated=39.2MB reserved=8526.0MB
train_loss: 1.117 train cer: 0.291
[Epoch 16] dev_loss_per_frame=1.0336, dev_CER=27.86%
allocated=39.2MB reserved=8526.0MB
train_loss: 1.098 train cer: 0.307
[Epoch 17] dev_loss_per_frame=1.0059, dev_CER=27.09%
  CER 改善，已保存到 saved_models/best_ctc_asr_cer_20251227-201828.pt (best_CER=27.09%)
allocated=39.8MB reserved=13158.0MB
train_loss: 1.067 train cer: 0.293
[Epoch 18] dev_loss_per_frame=1.0001, dev_CER=26.73%
  CER 改善，已保存到 saved_models/best_ctc_asr_cer_20251227-201828.pt (best_CER=26.73%)
allocated=39.2MB reserved=13170.0MB
train_loss: 1.042 train cer: 0.295
[Epoch 19] dev_loss_per_frame=0.9699, dev_CER=26.10%
  CER 改善，已保存到 saved_models/best_ctc_asr_cer_20251227-201828.pt (best_CER=26.10%)
allocated=39.5MB reserved=13170.0MB
train_loss: 1.038 train cer: 0.284
[Epoch 20] dev_loss_per_frame=0.9715, dev_CER=26.12%
allocated=39.2MB reserved=13170.0MB
train_loss: 1.043 train cer: 0.287
[Epoch 21] dev_loss_per_frame=0.9735, dev_CER=26.14%
allocated=39.2MB reserved=13170.0MB
train_loss: 1.019 train cer: 0.278
[Epoch 22] dev_loss_per_frame=0.9564, dev_CER=25.66%
  CER 改善，已保存到 saved_models/best_ctc_asr_cer_20251227-201828.pt (best_CER=25.66%)
allocated=39.3MB reserved=13170.0MB
train_loss: 1.004 train cer: 0.279
[Epoch 23] dev_loss_per_frame=0.9553, dev_CER=25.59%
  CER 改善，已保存到 saved_models/best_ctc_asr_cer_20251227-201828.pt (best_CER=25.59%)
allocated=39.9MB reserved=13170.0MB
train_loss: 0.997 train cer: 0.274
[Epoch 24] dev_loss_per_frame=0.9271, dev_CER=24.82%
  CER 改善，已保存到 saved_models/best_ctc_asr_cer_20251227-201828.pt (best_CER=24.82%)
allocated=40.1MB reserved=13170.0MB
train_loss: 0.999 train cer: 0.285
[Epoch 25] dev_loss_per_frame=0.9435, dev_CER=25.32%
allocated=39.2MB reserved=13170.0MB
train_loss: 0.974 train cer: 0.269
[Epoch 26] dev_loss_per_frame=0.9328, dev_CER=24.94%
allocated=39.2MB reserved=13170.0MB
train_loss: 0.951 train cer: 0.273
[Epoch 27] dev_loss_per_frame=0.9381, dev_CER=25.20%
allocated=39.2MB reserved=13172.0MB
train_loss: 0.954 train cer: 0.273
[Epoch 28] dev_loss_per_frame=0.9123, dev_CER=24.30%
  CER 改善，已保存到 saved_models/best_ctc_asr_cer_20251227-201828.pt (best_CER=24.30%)
allocated=40.1MB reserved=13172.0MB
train_loss: 0.946 train cer: 0.270
[Epoch 29] dev_loss_per_frame=0.9154, dev_CER=24.36%
allocated=39.2MB reserved=13172.0MB
train_loss: 0.950 train cer: 0.258
[Epoch 30] dev_loss_per_frame=0.9164, dev_CER=24.47%
allocated=39.2MB reserved=13172.0MB
train_loss: 0.947 train cer: 0.257
[Epoch 31] dev_loss_per_frame=0.9120, dev_CER=24.47%
allocated=39.3MB reserved=13172.0MB
train_loss: 0.925 train cer: 0.265
[Epoch 32] dev_loss_per_frame=0.9158, dev_CER=24.21%
  CER 改善，已保存到 saved_models/best_ctc_asr_cer_20251227-201828.pt (best_CER=24.21%)
allocated=39.5MB reserved=13172.0MB
train_loss: 0.928 train cer: 0.251
[Epoch 33] dev_loss_per_frame=0.9270, dev_CER=24.72%
allocated=39.3MB reserved=13172.0MB
train_loss: 0.925 train cer: 0.262
[Epoch 34] dev_loss_per_frame=0.8902, dev_CER=23.60%
  CER 改善，已保存到 saved_models/best_ctc_asr_cer_20251227-201828.pt (best_CER=23.60%)
allocated=39.5MB reserved=13172.0MB
train_loss: 0.909 train cer: 0.258
[Epoch 35] dev_loss_per_frame=0.8873, dev_CER=23.65%
allocated=39.3MB reserved=13172.0MB
train_loss: 0.893 train cer: 0.245
[Epoch 36] dev_loss_per_frame=0.9109, dev_CER=24.20%
allocated=39.2MB reserved=13172.0MB
train_loss: 0.893 train cer: 0.255
[Epoch 37] dev_loss_per_frame=0.9071, dev_CER=24.03%
allocated=39.5MB reserved=13172.0MB
train_loss: 0.909 train cer: 0.268
[Epoch 38] dev_loss_per_frame=0.8847, dev_CER=23.55%
  CER 改善，已保存到 saved_models/best_ctc_asr_cer_20251227-201828.pt (best_CER=23.55%)
allocated=39.2MB reserved=13174.0MB
train_loss: 0.907 train cer: 0.250
[Epoch 39] dev_loss_per_frame=0.8684, dev_CER=23.09%
  CER 改善，已保存到 saved_models/best_ctc_asr_cer_20251227-201828.pt (best_CER=23.09%)
allocated=39.2MB reserved=13174.0MB
train_loss: 0.883 train cer: 0.252
[Epoch 40] dev_loss_per_frame=0.8572, dev_CER=22.79%
  CER 改善，已保存到 saved_models/best_ctc_asr_cer_20251227-201828.pt (best_CER=22.79%)
allocated=39.8MB reserved=13174.0MB
train_loss: 0.884 train cer: 0.235
[Epoch 41] dev_loss_per_frame=0.8666, dev_CER=23.02%
allocated=39.2MB reserved=13174.0MB
train_loss: 0.876 train cer: 0.257
[Epoch 42] dev_loss_per_frame=0.8636, dev_CER=22.99%
allocated=39.6MB reserved=13174.0MB
train_loss: 0.878 train cer: 0.251
[Epoch 43] dev_loss_per_frame=0.8701, dev_CER=23.02%
allocated=39.2MB reserved=13174.0MB
train_loss: 0.866 train cer: 0.245
[Epoch 44] dev_loss_per_frame=0.8724, dev_CER=22.97%
old_lr=0.001, new_lr=0.0005
allocated=40.0MB reserved=13174.0MB
train_loss: 0.779 train cer: 0.233
[Epoch 45] dev_loss_per_frame=0.8048, dev_CER=21.13%
  CER 改善，已保存到 saved_models/best_ctc_asr_cer_20251227-201828.pt (best_CER=21.13%)
allocated=39.2MB reserved=13174.0MB
train_loss: 0.759 train cer: 0.215
[Epoch 46] dev_loss_per_frame=0.7978, dev_CER=20.86%
  CER 改善，已保存到 saved_models/best_ctc_asr_cer_20251227-201828.pt (best_CER=20.86%)
allocated=39.9MB reserved=13174.0MB
train_loss: 0.761 train cer: 0.214
[Epoch 47] dev_loss_per_frame=0.7991, dev_CER=20.96%
allocated=39.2MB reserved=13174.0MB
train_loss: 0.750 train cer: 0.212
[Epoch 48] dev_loss_per_frame=0.7997, dev_CER=20.98%
allocated=40.0MB reserved=13174.0MB
train_loss: 0.744 train cer: 0.216
[Epoch 49] dev_loss_per_frame=0.7950, dev_CER=20.76%
  CER 改善，已保存到 saved_models/best_ctc_asr_cer_20251227-201828.pt (best_CER=20.76%)
allocated=39.5MB reserved=13174.0MB
train_loss: 0.739 train cer: 0.204
[Epoch 50] dev_loss_per_frame=0.7895, dev_CER=20.76%
allocated=39.2MB reserved=13174.0MB
train_loss: 0.734 train cer: 0.213
[Epoch 51] dev_loss_per_frame=0.7846, dev_CER=20.50%
  CER 改善，已保存到 saved_models/best_ctc_asr_cer_20251227-201828.pt (best_CER=20.50%)
allocated=40.2MB reserved=13174.0MB
train_loss: 0.736 train cer: 0.221
[Epoch 52] dev_loss_per_frame=0.7909, dev_CER=20.73%
allocated=39.2MB reserved=13174.0MB
train_loss: 0.731 train cer: 0.215
[Epoch 53] dev_loss_per_frame=0.7921, dev_CER=20.76%
allocated=39.5MB reserved=13174.0MB
train_loss: 0.735 train cer: 0.214
[Epoch 54] dev_loss_per_frame=0.7870, dev_CER=20.60%
allocated=39.5MB reserved=13174.0MB
train_loss: 0.717 train cer: 0.201
[Epoch 55] dev_loss_per_frame=0.7905, dev_CER=20.63%
old_lr=0.0005, new_lr=0.00025
allocated=39.2MB reserved=13174.0MB
train_loss: 0.688 train cer: 0.207
[Epoch 56] dev_loss_per_frame=0.7660, dev_CER=19.83%
  CER 改善，已保存到 saved_models/best_ctc_asr_cer_20251227-201828.pt (best_CER=19.83%)
allocated=39.3MB reserved=13174.0MB
train_loss: 0.674 train cer: 0.191
[Epoch 57] dev_loss_per_frame=0.7667, dev_CER=19.76%
  CER 改善，已保存到 saved_models/best_ctc_asr_cer_20251227-201828.pt (best_CER=19.76%)
allocated=39.5MB reserved=13174.0MB
train_loss: 0.667 train cer: 0.186
[Epoch 58] dev_loss_per_frame=0.7643, dev_CER=19.70%
  CER 改善，已保存到 saved_models/best_ctc_asr_cer_20251227-201828.pt (best_CER=19.70%)
allocated=39.2MB reserved=13174.0MB
train_loss: 0.664 train cer: 0.197
[Epoch 59] dev_loss_per_frame=0.7610, dev_CER=19.61%
  CER 改善，已保存到 saved_models/best_ctc_asr_cer_20251227-201828.pt (best_CER=19.61%)
allocated=39.5MB reserved=13174.0MB
train_loss: 0.665 train cer: 0.193
[Epoch 60] dev_loss_per_frame=0.7627, dev_CER=19.56%
  CER 改善，已保存到 saved_models/best_ctc_asr_cer_20251227-201828.pt (best_CER=19.56%)
allocated=39.7MB reserved=13174.0MB
train_loss: 0.660 train cer: 0.198
[Epoch 61] dev_loss_per_frame=0.7662, dev_CER=19.71%
allocated=40.2MB reserved=13174.0MB
train_loss: 0.662 train cer: 0.194
[Epoch 62] dev_loss_per_frame=0.7545, dev_CER=19.44%
  CER 改善，已保存到 saved_models/best_ctc_asr_cer_20251227-201828.pt (best_CER=19.44%)
allocated=39.2MB reserved=13174.0MB
train_loss: 0.660 train cer: 0.187
[Epoch 63] dev_loss_per_frame=0.7595, dev_CER=19.51%
allocated=39.2MB reserved=13174.0MB
train_loss: 0.658 train cer: 0.203
[Epoch 64] dev_loss_per_frame=0.7564, dev_CER=19.49%
allocated=39.7MB reserved=13174.0MB
train_loss: 0.654 train cer: 0.201
[Epoch 65] dev_loss_per_frame=0.7590, dev_CER=19.48%
allocated=39.2MB reserved=13174.0MB
train_loss: 0.654 train cer: 0.185
[Epoch 66] dev_loss_per_frame=0.7593, dev_CER=19.53%
old_lr=0.00025, new_lr=0.000125
allocated=39.5MB reserved=13174.0MB
train_loss: 0.635 train cer: 0.181
[Epoch 67] dev_loss_per_frame=0.7504, dev_CER=19.14%
  CER 改善，已保存到 saved_models/best_ctc_asr_cer_20251227-201828.pt (best_CER=19.14%)
allocated=39.2MB reserved=13174.0MB
train_loss: 0.627 train cer: 0.177
[Epoch 68] dev_loss_per_frame=0.7485, dev_CER=19.09%
  CER 改善，已保存到 saved_models/best_ctc_asr_cer_20251227-201828.pt (best_CER=19.09%)
allocated=39.2MB reserved=13174.0MB
train_loss: 0.627 train cer: 0.192
[Epoch 69] dev_loss_per_frame=0.7476, dev_CER=18.99%
  CER 改善，已保存到 saved_models/best_ctc_asr_cer_20251227-201828.pt (best_CER=18.99%)
allocated=39.2MB reserved=13174.0MB
train_loss: 0.624 train cer: 0.184
[Epoch 70] dev_loss_per_frame=0.7460, dev_CER=18.98%
  CER 改善，已保存到 saved_models/best_ctc_asr_cer_20251227-201828.pt (best_CER=18.98%)
allocated=39.2MB reserved=13174.0MB
train_loss: 0.624 train cer: 0.170
[Epoch 71] dev_loss_per_frame=0.7473, dev_CER=19.00%
allocated=39.2MB reserved=13174.0MB
train_loss: 0.627 train cer: 0.183
[Epoch 72] dev_loss_per_frame=0.7467, dev_CER=19.01%
allocated=39.2MB reserved=9898.0MB
train_loss: 0.619 train cer: 0.193
[Epoch 73] dev_loss_per_frame=0.7483, dev_CER=19.08%
allocated=40.0MB reserved=9898.0MB
train_loss: 0.617 train cer: 0.182
[Epoch 74] dev_loss_per_frame=0.7518, dev_CER=19.08%
old_lr=0.000125, new_lr=6.25e-05
allocated=39.5MB reserved=9904.0MB
train_loss: 0.612 train cer: 0.178
[Epoch 75] dev_loss_per_frame=0.7426, dev_CER=18.80%
  CER 改善，已保存到 saved_models/best_ctc_asr_cer_20251227-201828.pt (best_CER=18.80%)
allocated=39.2MB reserved=9908.0MB
train_loss: 0.608 train cer: 0.163
[Epoch 76] dev_loss_per_frame=0.7428, dev_CER=18.82%
allocated=39.5MB reserved=9908.0MB
train_loss: 0.604 train cer: 0.180
[Epoch 77] dev_loss_per_frame=0.7424, dev_CER=18.77%
  CER 改善，已保存到 saved_models/best_ctc_asr_cer_20251227-201828.pt (best_CER=18.77%)
allocated=39.3MB reserved=9908.0MB
train_loss: 0.604 train cer: 0.175
[Epoch 78] dev_loss_per_frame=0.7394, dev_CER=18.70%
  CER 改善，已保存到 saved_models/best_ctc_asr_cer_20251227-201828.pt (best_CER=18.70%)
allocated=39.7MB reserved=9908.0MB
train_loss: 0.603 train cer: 0.175
[Epoch 79] dev_loss_per_frame=0.7429, dev_CER=18.79%
allocated=39.6MB reserved=9916.0MB
train_loss: 0.598 train cer: 0.172
[Epoch 80] dev_loss_per_frame=0.7421, dev_CER=18.70%
allocated=39.2MB reserved=9916.0MB
train_loss: 0.598 train cer: 0.173
[Epoch 81] dev_loss_per_frame=0.7415, dev_CER=18.74%
allocated=39.2MB reserved=9916.0MB
train_loss: 0.597 train cer: 0.170
[Epoch 82] dev_loss_per_frame=0.7422, dev_CER=18.72%
old_lr=6.25e-05, new_lr=3.125e-05
allocated=39.2MB reserved=9916.0MB
train_loss: 0.592 train cer: 0.165
[Epoch 83] dev_loss_per_frame=0.7395, dev_CER=18.67%
  CER 改善，已保存到 saved_models/best_ctc_asr_cer_20251227-201828.pt (best_CER=18.67%)
allocated=39.2MB reserved=9926.0MB
train_loss: 0.597 train cer: 0.174
[Epoch 84] dev_loss_per_frame=0.7399, dev_CER=18.61%
  CER 改善，已保存到 saved_models/best_ctc_asr_cer_20251227-201828.pt (best_CER=18.61%)
allocated=39.2MB reserved=9926.0MB
train_loss: 0.593 train cer: 0.179
[Epoch 85] dev_loss_per_frame=0.7409, dev_CER=18.66%
allocated=39.2MB reserved=9926.0MB
train_loss: 0.594 train cer: 0.168
[Epoch 86] dev_loss_per_frame=0.7394, dev_CER=18.65%
old_lr=3.125e-05, new_lr=1.5625e-05
allocated=39.2MB reserved=9926.0MB
train_loss: 0.590 train cer: 0.173
[Epoch 87] dev_loss_per_frame=0.7398, dev_CER=18.56%
  CER 改善，已保存到 saved_models/best_ctc_asr_cer_20251227-201828.pt (best_CER=18.56%)
allocated=39.5MB reserved=9926.0MB
train_loss: 0.593 train cer: 0.167
[Epoch 88] dev_loss_per_frame=0.7390, dev_CER=18.60%
allocated=39.5MB reserved=9926.0MB
train_loss: 0.587 train cer: 0.175
[Epoch 89] dev_loss_per_frame=0.7401, dev_CER=18.60%
allocated=39.2MB reserved=9926.0MB
train_loss: 0.583 train cer: 0.172
[Epoch 90] dev_loss_per_frame=0.7394, dev_CER=18.60%
allocated=39.5MB reserved=9926.0MB
train_loss: 0.590 train cer: 0.182
[Epoch 91] dev_loss_per_frame=0.7387, dev_CER=18.55%
  CER 改善，已保存到 saved_models/best_ctc_asr_cer_20251227-201828.pt (best_CER=18.55%)
allocated=39.5MB reserved=9926.0MB
train_loss: 0.588 train cer: 0.174
[Epoch 92] dev_loss_per_frame=0.7396, dev_CER=18.60%
allocated=39.2MB reserved=9926.0MB
train_loss: 0.587 train cer: 0.173
[Epoch 93] dev_loss_per_frame=0.7380, dev_CER=18.58%
allocated=39.5MB reserved=9926.0MB
train_loss: 0.588 train cer: 0.174
[Epoch 94] dev_loss_per_frame=0.7373, dev_CER=18.56%
allocated=39.2MB reserved=9926.0MB
train_loss: 0.587 train cer: 0.178
[Epoch 95] dev_loss_per_frame=0.7389, dev_CER=18.58%
allocated=39.3MB reserved=9926.0MB
train_loss: 0.580 train cer: 0.173
[Epoch 96] dev_loss_per_frame=0.7372, dev_CER=18.55%
  CER 改善，已保存到 saved_models/best_ctc_asr_cer_20251227-201828.pt (best_CER=18.55%)
allocated=39.2MB reserved=9926.0MB
train_loss: 0.585 train cer: 0.171
[Epoch 97] dev_loss_per_frame=0.7394, dev_CER=18.54%
  CER 改善，已保存到 saved_models/best_ctc_asr_cer_20251227-201828.pt (best_CER=18.54%)
allocated=39.2MB reserved=9926.0MB
train_loss: 0.586 train cer: 0.176
[Epoch 98] dev_loss_per_frame=0.7387, dev_CER=18.59%
old_lr=1.5625e-05, new_lr=7.8125e-06
allocated=39.2MB reserved=9926.0MB
train_loss: 0.582 train cer: 0.161
[Epoch 99] dev_loss_per_frame=0.7368, dev_CER=18.56%
allocated=39.2MB reserved=9926.0MB
train_loss: 0.584 train cer: 0.185
[Epoch 100] dev_loss_per_frame=0.7365, dev_CER=18.53%
  CER 改善，已保存到 saved_models/best_ctc_asr_cer_20251227-201828.pt (best_CER=18.53%)
allocated=39.2MB reserved=9926.0MB
train_loss: 0.588 train cer: 0.169
[Epoch 101] dev_loss_per_frame=0.7389, dev_CER=18.58%
allocated=39.5MB reserved=9926.0MB
train_loss: 0.582 train cer: 0.168
[Epoch 102] dev_loss_per_frame=0.7375, dev_CER=18.59%
allocated=39.3MB reserved=9926.0MB
train_loss: 0.582 train cer: 0.174
[Epoch 103] dev_loss_per_frame=0.7369, dev_CER=18.54%
allocated=39.6MB reserved=9926.0MB
train_loss: 0.589 train cer: 0.174
[Epoch 104] dev_loss_per_frame=0.7371, dev_CER=18.54%
old_lr=7.8125e-06, new_lr=3.90625e-06
allocated=39.3MB reserved=9926.0MB
train_loss: 0.581 train cer: 0.163
[Epoch 105] dev_loss_per_frame=0.7376, dev_CER=18.56%
allocated=39.2MB reserved=9926.0MB
train_loss: 0.585 train cer: 0.180
[Epoch 106] dev_loss_per_frame=0.7396, dev_CER=18.55%
allocated=39.5MB reserved=9926.0MB
train_loss: 0.586 train cer: 0.178
[Epoch 107] dev_loss_per_frame=0.7369, dev_CER=18.53%
  CER 改善，已保存到 saved_models/best_ctc_asr_cer_20251227-201828.pt (best_CER=18.53%)
allocated=39.2MB reserved=9926.0MB
train_loss: 0.579 train cer: 0.169
[Epoch 108] dev_loss_per_frame=0.7379, dev_CER=18.56%
old_lr=3.90625e-06, new_lr=1.953125e-06
allocated=39.2MB reserved=9926.0MB
train_loss: 0.585 train cer: 0.166
[Epoch 109] dev_loss_per_frame=0.7385, dev_CER=18.56%
allocated=40.2MB reserved=9926.0MB
train_loss: 0.586 train cer: 0.168
[Epoch 110] dev_loss_per_frame=0.7385, dev_CER=18.54%
allocated=39.5MB reserved=9926.0MB
train_loss: 0.582 train cer: 0.169
[Epoch 111] dev_loss_per_frame=0.7398, dev_CER=18.57%
allocated=39.2MB reserved=9926.0MB
train_loss: 0.585 train cer: 0.165
[Epoch 112] dev_loss_per_frame=0.7373, dev_CER=18.53%
old_lr=1.953125e-06, new_lr=9.765625e-07
allocated=39.5MB reserved=9926.0MB
train_loss: 0.583 train cer: 0.182
[Epoch 113] dev_loss_per_frame=0.7385, dev_CER=18.56%
allocated=39.2MB reserved=9926.0MB
train_loss: 0.580 train cer: 0.170
[Epoch 114] dev_loss_per_frame=0.7376, dev_CER=18.56%
allocated=39.2MB reserved=9926.0MB
train_loss: 0.582 train cer: 0.167
[Epoch 115] dev_loss_per_frame=0.7381, dev_CER=18.56%
allocated=40.1MB reserved=9926.0MB
train_loss: 0.580 train cer: 0.178
[Epoch 116] dev_loss_per_frame=0.7377, dev_CER=18.56%
old_lr=9.765625e-07, new_lr=4.8828125e-07
allocated=39.7MB reserved=9926.0MB
train_loss: 0.581 train cer: 0.161
[Epoch 117] dev_loss_per_frame=0.7372, dev_CER=18.54%
allocated=39.2MB reserved=9926.0MB
